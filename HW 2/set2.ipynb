{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problems 1-2\n",
    "\n",
    "def flip_single_coin_n_times(n = 10):\n",
    "    \"\"\"\n",
    "    binomial 1 means getting heads,\n",
    "    binomial 0 means getting tails.\n",
    "    We assume fair coin.\n",
    "    We return the proportion of flips returning heads.\n",
    "    \"\"\"\n",
    "    #flip one fair coin n times\n",
    "    flips = np.random.binomial(1, .5, n)\n",
    "    heads_fraction = np.sum(flips) / n\n",
    "    return heads_fraction\n",
    "\n",
    "def coin_experiment_single_run(n = 10, coin_count = 1000):\n",
    "    #for c_rand\n",
    "    random_index = np.random.randint(coin_count, size = 1)\n",
    "    #list of size coin_count, for v of each coin\n",
    "    heads_fraction_all = []\n",
    "    \n",
    "    for coin in range(coin_count):\n",
    "        heads_fraction_single = flip_single_coin_n_times()\n",
    "        heads_fraction_all.append(heads_fraction_single)\n",
    "        \n",
    "        #find v_0 and v_rand, if applicable\n",
    "        if coin == 0:\n",
    "            v_0 = heads_fraction_single\n",
    "        #cannot use elif, since random_index might be 0\n",
    "        if coin == random_index:\n",
    "            v_rand = heads_fraction_single\n",
    "    \n",
    "    #find v_min\n",
    "    heads_fraction_all = np.asarray(heads_fraction_all)\n",
    "    v_min = np.min(heads_fraction_all)\n",
    "    return (v_0, v_rand, v_min)\n",
    "\n",
    "def coin_experiment_expected_values(n = 10, coin_count = 1000, runs = 100000):\n",
    "    \"\"\"\n",
    "    run coin_experiment_single_run runs = 1000 times\n",
    "    Outputs:\n",
    "        v_0_expected (should be around .5)\n",
    "        v_rand_expected (should be around .5)\n",
    "        v_min_expected (should be a lot less than .5)\n",
    "    \"\"\"\n",
    "    v_0_all = []\n",
    "    v_rand_all = []\n",
    "    v_min_all = []\n",
    "    for run in range(runs):\n",
    "        if run % 10000 == 0:\n",
    "            print (\"on run \", run)\n",
    "        v_0, v_rand, v_min = coin_experiment_single_run(n = n, coin_count = coin_count)\n",
    "        v_0_all.append(v_0)\n",
    "        v_rand_all.append(v_rand)\n",
    "        v_min_all.append(v_min)\n",
    "    \n",
    "    #convert to ndarray\n",
    "    v_0_all = np.asarray(v_0_all)\n",
    "    v_rand_all = np.asarray(v_rand_all)\n",
    "    v_min_all = np.asarray(v_min_all)\n",
    "    \n",
    "    v_0_expected = np.sum(v_0_all) / runs\n",
    "    v_rand_expected = np.sum(v_rand_all) / runs\n",
    "    v_min_expected = np.sum(v_min_all) / runs\n",
    "    \n",
    "    return (v_0_expected, v_rand_expected, v_min_expected)\n",
    "\n",
    "v_0_expected, v_rand_expected, v_min_expected = coin_experiment_expected_values()\n",
    "print(\"Problems 1-2\")\n",
    "print (\"expected v_0 is\", v_0_expected, \"\\n\")\n",
    "print (\"expected v_rand is\", v_rand_expected, \"\\n\")\n",
    "print (\"expected v_min is\", v_min_expected, \"\\n\")\n",
    "\n",
    "#example output:\n",
    "#Problems 1-2\n",
    "#expected v_0 is 0.4994809999999999 \n",
    "\n",
    "#expected v_rand is 0.4993659999999999 \n",
    "\n",
    "#expected v_min is 0.03745199999999999 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems 5/6\n",
      "In sample error is 0.04\n",
      "Out of sample error is 0.06\n",
      "\n",
      "\n",
      "Problem 7\n",
      "Iterations until convergence: 4.177\n"
     ]
    }
   ],
   "source": [
    "#problems 5-7\n",
    "def generate_point(boundary1, boundary2, dimension):\n",
    "    \"\"\"\n",
    "    Generate random two-dimensional point on \n",
    "    [boundary1, boundary 2] X [boundary1, boundary2] space.\n",
    "    Returns ndarray of (x, y) point\n",
    "    \"\"\"\n",
    "    random_point = np.zeros(dimension)\n",
    "    for i in range(dimension):\n",
    "        random_point[i] = np.random.uniform(boundary1, boundary2, 1)\n",
    "    return random_point\n",
    "\n",
    "def generate_target_f_linear():\n",
    "    \"\"\"\n",
    "    Returns slope and intercept of line connecting two random points\n",
    "    \"\"\"\n",
    "    point_1 = generate_point(-1, 1, 2)\n",
    "    point_2 = generate_point(-1, 1, 2)\n",
    "    \n",
    "    # slope = (y2 - y1) / (x2 - x1)\n",
    "    slope = (point_2[1] - point_1[1]) / (point_2[0] - point_1[0])\n",
    "    # intercept = y1 - slope * x1\n",
    "    intercept = point_2[1] - slope * point_2[0]\n",
    "    return(slope, intercept)\n",
    "\n",
    "\n",
    "def classify_point_linear(random_point, slope, intercept):\n",
    "    \"\"\"\n",
    "    Given random_point in (x, y) form and a slope and intercept,\n",
    "    label the point\n",
    "    +1 if it falls above the line\n",
    "    -1 if it falls below the line\n",
    "    \"\"\"\n",
    "    if random_point[1] > slope * random_point[0] + intercept:\n",
    "        classification = 1\n",
    "    else:\n",
    "        classification = -1\n",
    "    return classification\n",
    "\n",
    "\n",
    "def create_data_linear(N, slope, intercept):\n",
    "    \"\"\"\n",
    "    Creates N points for testing data using target f(X) = slope * X + intercept;\n",
    "    Notes:\n",
    "        Target function f is a line connecting two points in X space [-1, 1] x [-1, 1]\n",
    "        Classification is based on whether points lie above or below f\n",
    "        \n",
    "    Inputs:\n",
    "        N (int)\n",
    "        slope (float), from generate_target_f_linear\n",
    "        intercept (float), from generate_target_f_linear\n",
    "        \n",
    "    Outputs:\n",
    "        X (ndarray), X.shape = (N, 3); x0 = 1\n",
    "        Y (ndarray), Y.shsape = (N, )\n",
    "    \"\"\"\n",
    "    # create matrix X, where x0 is always 1, to accomodate w0\n",
    "    X = np.ones((N, 3))\n",
    "    Y = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        random_point = generate_point(-1, 1, 2)\n",
    "        classification = classify_point_linear(random_point, slope, intercept)\n",
    "        X[i, 1:3] = random_point\n",
    "        Y[i] = classification\n",
    "        \n",
    "    return (X, Y)\n",
    "\n",
    "\n",
    "def linear_regression_weights(X_training, Y_training):\n",
    "    \"\"\"\n",
    "    Outputs:\n",
    "        weights (ndarray), weights.shape = d + 1 (one-dimensional)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_pseudo_inverse = np.linalg.pinv(X_training)\n",
    "    weights = np.dot(X_pseudo_inverse, Y_training)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def label_Y_g(X_training, weights):\n",
    "    Y_g = np.sign(np.dot(X_training, weights))\n",
    "    return Y_g\n",
    "    \n",
    "\n",
    "def error_linear(Y_f, Y_g):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        Y_f: (ndarray) Y_f.shape = (N, ); Y labels generated by target func\n",
    "        Y_g: (ndarray) Y_g.shape = (N, ); Y labels generated by hypothesis\n",
    "    \"\"\"\n",
    "    if Y_f.shape[0] != Y_g.shape[0]:\n",
    "        raise RunTimeError('Y_f and Y_g are not the same length')\n",
    "    correct_count = np.sum(Y_f == Y_g)\n",
    "    error_percentage = (Y_f.shape[0] - correct_count) / Y_f.shape[0]\n",
    "    return error_percentage\n",
    "\n",
    "\n",
    "def perceptron(X_training, Y_training, weight = np.zeros(3)):\n",
    "    \"\"\"\n",
    "    Starts with the weight as a zero-vector.\n",
    "    In a given iteration, checks every single point for misclassification.\n",
    "    At the end of the iteration, randomly chooses a misclassified point for weight adjustment.\n",
    "    Returns the new weight once all the points are correctly classified.\n",
    "    \n",
    "    Inputs:\n",
    "        X_training (ndarray), complete input features of all points\n",
    "        Y_training (ndarray), complete training classification of all points\n",
    "        \n",
    "    Outputs:\n",
    "        weight (ndarray), final hypothesis g(X) = Y for perceptron learning\n",
    "        iteration_count (int), number of iterations for convergence between g and f\n",
    "        \n",
    "    \"\"\"\n",
    "    iteration_count = 0\n",
    "    \n",
    "    while True:\n",
    "        misclassified_points = []\n",
    "        for i in range(len(X_training)):\n",
    "            if isit_misclassified_point(X_training[i], Y_training[i], weight) == True:\n",
    "                misclassified_points.append(i)\n",
    "        if len(misclassified_points) == 0:\n",
    "            break\n",
    "            \n",
    "        random_i = random.choice(misclassified_points)\n",
    "        weight = adjust_weight(X_training[random_i], Y_training[random_i], weight)\n",
    "        iteration_count += 1\n",
    "        \n",
    "    return (weight, iteration_count)\n",
    "\n",
    "def adjust_weight(x_misclassified, y_misclassified, weight):\n",
    "    \"\"\"\n",
    "    Given a single misclassified point and the current weight vector,\n",
    "    adjust the weight to accomodate our misclassifed point.\n",
    "    \n",
    "    Inputs:\n",
    "        x_misclassified (ndarray)\n",
    "        y_misclassified (+/- 1)\n",
    "        weight (ndarray)\n",
    "        \n",
    "    Outputs:\n",
    "        misclassified (boolean)\n",
    "    \"\"\"\n",
    "    adjusted_weight = weight + np.dot(y_misclassified, x_misclassified)\n",
    "    return adjusted_weight\n",
    "\n",
    "\n",
    "def isit_misclassified_point(x, y, weight):\n",
    "    \"\"\"\n",
    "    Given a single point (i.e. vector x and label y)\n",
    "    and a weight that we are currently training or testing, it will determine whether the\n",
    "    current hypothesis weight correctly or incorrectly classifies the point.\n",
    "    Tests hypothesis g(x) = sign (weight . x) == y for classification\n",
    "    This is primarily used for pla, since there is a more computationally friendly\n",
    "    way for linear regression.\n",
    "    \n",
    "    Inputs:\n",
    "        x (ndarray)\n",
    "        y (+/- 1)\n",
    "        weight (must be ndarray)\n",
    "        \n",
    "    Outputs:\n",
    "        misclassified (boolean)\n",
    "    \n",
    "    \"\"\"\n",
    "    if np.sign(np.dot(weight.T, x)) != y:\n",
    "        miclassified = True\n",
    "    else:\n",
    "        miclassified = False\n",
    "    return miclassified\n",
    "\n",
    "def experiment_5(N_training, N_testing, runs = 1000):\n",
    "    for run in range(runs):\n",
    "        #generate linear target function on [-1, 1] x [-1, 1]\n",
    "        slope, intercept = generate_target_f_linear()\n",
    "        \n",
    "        #generate training, and then one-step learning\n",
    "        X_training, Y_training = create_data_linear(N_training, slope, intercept)\n",
    "        weights = linear_regression_weights(X_training, Y_training)\n",
    "        \n",
    "        #how does g label Y_training?\n",
    "        Y_training_g = label_Y_g(X_training, weights)\n",
    "        \n",
    "        #calculate in-sample error\n",
    "        e_in = error_linear(Y_training, Y_training_g)\n",
    "        \n",
    "        #generate testing data\n",
    "        X_testing, Y_testing = create_data_linear(N_training, slope, intercept)\n",
    "        \n",
    "        #how does g label Y_testing?\n",
    "        Y_testing_g = label_Y_g(X_testing, weights)\n",
    "        \n",
    "        #calculate out-of-sample error\n",
    "        e_out = error_linear(Y_testing, Y_testing_g)\n",
    "        \n",
    "    return (e_in, e_out)\n",
    "\n",
    "def experiment_7(N_training, runs = 1000):\n",
    "    iterations_tally = 0\n",
    "    for run in range(runs):\n",
    "        #generate linear target function on [-1, 1] x [-1, 1]\n",
    "        slope, intercept = generate_target_f_linear()\n",
    "        \n",
    "        #generate training, and then one-step learning\n",
    "        X_training, Y_training = create_data_linear(N_training, slope, intercept)\n",
    "        weights = linear_regression_weights(X_training, Y_training)\n",
    "        _, iterations = perceptron(X_training, Y_training, weight = weights)\n",
    "        iterations_tally += iterations\n",
    "    average_iterations = iterations_tally / runs\n",
    "    return average_iterations\n",
    "\n",
    "#homework answers\n",
    "\n",
    "print(\"Problems 5/6\")\n",
    "error_in, error_out = experiment_5(100, 1000, runs = 1000)\n",
    "print(\"In sample error is\", error_in)\n",
    "print(\"Out of sample error is\", error_out)\n",
    "print(\"\\n\")\n",
    "print(\"Problem 7\")\n",
    "print(\"Iterations until convergence:\", experiment_7(10, runs = 1000))\n",
    "\n",
    "#example output:\n",
    "#Problems 5/6\n",
    "#In sample error is 0.03\n",
    "#Out of sample error is 0.01\n",
    "\n",
    "#Problem 7\n",
    "#Iterations until convergence: 3.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 8\n",
      "Problems 9/10\n",
      "Regression equation \n",
      " -0.9928129029169119 + -0.0009431377749000227x_1 + 0.001174531635834422x_2 + -0.0027519898287829176x_1*x_2 + 1.5549160903595107x_1**2 + 1.5611160733958145x_2**2\n",
      "Average out of sample error 0.12664100000000014\n"
     ]
    }
   ],
   "source": [
    "#problems 8-10\n",
    "\n",
    "def my_target_function(x_1, x_2):\n",
    "    return (x_1**2 + x_2**2 - 0.6)\n",
    "\n",
    "\n",
    "def classify_point(random_point, my_target_function):\n",
    "    \"\"\"\n",
    "    Given random_point in (x_1, x_2) form and a target function,\n",
    "    label the point\n",
    "    +1 if my_target_function(x_1, x_1) > 0\n",
    "    -1 if my_target_function(x_1, x_1) < 0\n",
    "    Inputs:\n",
    "        random_point (x_1, x_2)\n",
    "        my_target_function(*random_point) represents a graph\n",
    "        Note that there are no *kwargs because random_point provides the input\n",
    "    \"\"\"\n",
    "    if np.sign(my_target_function(*random_point)) > 0:\n",
    "        classification = 1\n",
    "    else:\n",
    "        classification = -1\n",
    "    return classification\n",
    "\n",
    "\n",
    "def create_data(N, my_target_function):\n",
    "    \"\"\"\n",
    "    Creates N points for testing data using my_target_function that has been defined elsewhere\n",
    "    Notes:\n",
    "        Target function f is a function on X space [-1, 1] x [-1, 1]\n",
    "        Classification y is based on the sign of f(x)\n",
    "        \n",
    "    Inputs:\n",
    "        N (int)\n",
    "        my_target_function(*kwargs) represents a graph\n",
    "        \n",
    "    Outputs:\n",
    "        X (ndarray), X.shape = (N, 3); x0 = 1\n",
    "        Y (ndarray), Y.shsape = (N, )\n",
    "    \"\"\"\n",
    "    \n",
    "    # create matrix X, where x0 is always 1, to accomodate w0\n",
    "    X = np.ones((N, 3))\n",
    "    Y = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        random_point = generate_point(-1, 1, 2)\n",
    "        classification = classify_point(random_point, my_target_function)\n",
    "        X[i, 1:3] = random_point\n",
    "        Y[i] = classification\n",
    "        \n",
    "    return (X, Y)\n",
    "\n",
    "\n",
    "def create_noise(Y_training, percent_noise):\n",
    "    #calculate number of classifications to flip\n",
    "    N = Y_training.shape[0]\n",
    "    flip_count = int(N * percent_noise)\n",
    "    \n",
    "    #determine random indices of Y_training to flip\n",
    "    flip_inds = np.random.choice(range(N), size = flip_count, replace = False)\n",
    "    \n",
    "    #flip using fancy numpy indexing\n",
    "    Y_training[flip_inds] = -Y_training[flip_inds]\n",
    "    return Y_training\n",
    "\n",
    "\n",
    "def linear_regression_weights(X_training, Y_training):\n",
    "    \"\"\"\n",
    "    Outputs:\n",
    "        weights (ndarray), weights.shape = d + 1 (one-dimensional)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_pseudo_inverse = np.linalg.pinv(X_training)\n",
    "    weights = np.dot(X_pseudo_inverse, Y_training)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def X_to_Z_matrix(X_training, non_linear_transform):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X_training (ndarray), X_training.shape = (N, d+1);\n",
    "            has feature vector (1, x_1, x_2)\n",
    "    \n",
    "    Outputs:\n",
    "        Z_training (ndarray), Z_training.shape = (N, 6)\n",
    "    \"\"\"\n",
    "    N = X_training.shape[0]\n",
    "    Z_training = np.ones([N, 6])\n",
    "    \n",
    "    #transform the matrix, row by row using non_linear_transform\n",
    "    for i in range(N):\n",
    "        Z_training[i, :] = non_linear_transform(X_training[i])\n",
    "        \n",
    "    return Z_training\n",
    "\n",
    "\n",
    "def non_linear_transform(x):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x: (ndarray), x.shape = d + 1 (one dimensional)\n",
    "    Outputs:\n",
    "        z: (ndarray), z.shape = 6 (one dimensional)\n",
    "        \n",
    "    Nonlinear feature vector:\n",
    "        (1, x_1, x_2, x_1 * x_2, x_1**2, x_2**2)\n",
    "    \n",
    "    \"\"\"\n",
    "    _, x_1, x_2 = x\n",
    "    z = [1, x_1, x_2, x_1 * x_2, x_1**2, x_2**2]\n",
    "    z = np.asarray(z)\n",
    "    return z\n",
    "\n",
    "\n",
    "def experiment_8(N = 1000, runs = 1000):\n",
    "    error_in_tally = 0\n",
    "    for run in range(runs):\n",
    "        #create training data\n",
    "        X_training, Y_training = create_data(N, my_target_function)\n",
    "\n",
    "        #add 10% noise\n",
    "        Y_training = create_noise(Y_training, .1)\n",
    "\n",
    "        #linear regression\n",
    "        weights = linear_regression_weights(X_training, Y_training)\n",
    "\n",
    "        #how would our linear regression label Y_training?\n",
    "        Y_training_g = label_Y_g(X_training, weights)\n",
    "\n",
    "        #calculate in sample error\n",
    "        error_in = error_linear(Y_training, Y_training_g)\n",
    "        \n",
    "        #add to tally\n",
    "        error_in_tally += error_in\n",
    "    error_in_avg = error_in_tally / runs\n",
    "    return error_in_avg\n",
    "\n",
    "def experiment_9(N_training = 1000, N_testing = 1000, runs = 1000):\n",
    "    error_out_tally = 0\n",
    "    weights_tally = []\n",
    "    for run in range(runs):\n",
    "        #create training data\n",
    "        X_training, Y_training = create_data(N_training, my_target_function)\n",
    "\n",
    "        #add 10% noise\n",
    "        Y_training = create_noise(Y_training, .1)\n",
    "        \n",
    "        #nonlinear transformation to Z-space\n",
    "        Z_training = X_to_Z_matrix(X_training, non_linear_transform)\n",
    "\n",
    "        #linear regression and adding to tally\n",
    "        weights = linear_regression_weights(Z_training, Y_training)\n",
    "        weights_tally.append(weights)\n",
    "        \n",
    "        #generate testing data\n",
    "        X_testing, Y_testing = create_data(N_testing, my_target_function)\n",
    "        \n",
    "        #add 10% noise to testing data\n",
    "        Y_testing = create_noise(Y_testing, .1)\n",
    "        \n",
    "        #nonlinear transformation to Z-space\n",
    "        Z_testing = X_to_Z_matrix(X_testing, non_linear_transform)\n",
    "        \n",
    "        #how would our linear regression label Y_training?\n",
    "        Y_testing_g = label_Y_g(Z_testing, weights)\n",
    "\n",
    "        #calculate out of sample error\n",
    "        error_out = error_linear(Y_testing, Y_testing_g)\n",
    "        \n",
    "        #add to tally\n",
    "        error_out_tally += error_out\n",
    "        \n",
    "    #getting averages    \n",
    "    error_out_avg = error_out_tally / runs\n",
    "    \n",
    "    average_weights = []\n",
    "    weights_tally = np.asarray(weights_tally)\n",
    "    for i in range(6):\n",
    "        mean_weight = np.mean(weights_tally.T[i])\n",
    "        average_weights.append(mean_weight)\n",
    "    average_weights = np.asarray(average_weights)\n",
    "    \n",
    "    return error_out_avg, average_weights\n",
    "\n",
    "print(\"Problem 8\")\n",
    "print(\"Average in-sample error is\", experiment_8())\n",
    "\n",
    "#sample output\n",
    "#Problem 8\n",
    "#Average in-sample error is 0.5073309999999999\n",
    "\n",
    "\n",
    "print(\"Problems 9/10\")\n",
    "error_out_avg, c = experiment_9(runs = 1000)\n",
    "regression = f\"{c[0]} + {c[1]}x_1 + {c[2]}x_2 + {c[3]}x_1*x_2 + {c[4]}x_1**2 + {c[5]}x_2**2\"\n",
    "print (\"Regression equation \\n\", regression)\n",
    "print (\"Average out of sample error\", error_out_avg)\n",
    "\n",
    "#sample output\n",
    "#Problems 9/10\n",
    "#Regression equation \n",
    "#-0.9928129029169119 + -0.0009431377749000227x_1 + 0.001174531635834422x_2 + -0.0027519898287829176x_1*x_2 + 1.5549160903595107x_1**2 + 1.5611160733958145x_2**2\n",
    "#Average out of sample error 0.12664100000000014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
