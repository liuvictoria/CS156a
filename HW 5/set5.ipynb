{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#create random number generator\n",
    "rg = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: 0.04473629039778207\n",
      "v: 0.023958714099141746\n",
      "iterations: 10\n",
      "\n",
      "error for coordinate descent: 0.13981379199615315\n"
     ]
    }
   ],
   "source": [
    "#problems 5-7\n",
    "\n",
    "def nonlinear_error(u, v):\n",
    "    \"\"\"\n",
    "    as described in the problem\n",
    "    \"\"\"\n",
    "    return (u * np.exp(v) - 2 * v * np.exp(-u)) ** 2\n",
    "\n",
    "def dE_du(u, v):\n",
    "    \"\"\"\n",
    "    the direction of steepest increase in the u direction\n",
    "    \"\"\"\n",
    "    return 2 * (np.exp(v) + 2 * v * np.exp(-u)) * (u * np.exp(v) - 2 * v * np.exp(-u))\n",
    "\n",
    "def dE_dv(u, v):\n",
    "    \"\"\"\n",
    "    the direction of steepest increase in the v direction\n",
    "    \"\"\"\n",
    "    return 2 * (u * np.exp(v) - 2 * np.exp(-u)) * (u * np.exp(v) - 2 * v * np.exp(-u))\n",
    "\n",
    "def gradient_descent(u = 1, v = 1, eta = .1, error_thres = 10**(-14)):\n",
    "    \"\"\"\n",
    "    gradient descent with the given parameters\n",
    "    start at u = 1 and v = 1\n",
    "    outputs u, v, and the iterations to get below a certain error threshold\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    error = nonlinear_error(u, v)\n",
    "    while error > error_thres:\n",
    "        du = dE_du(u, v)\n",
    "        dv = dE_dv(u, v)\n",
    "        u -= eta * du\n",
    "        v -= eta * dv\n",
    "        error = nonlinear_error(u, v)\n",
    "        iteration += 1\n",
    "    return (u, v, iteration)\n",
    "\n",
    "\n",
    "#problem 7\n",
    "def coordinate_descent(u = 1, v = 1, eta = .1, stop = 15):\n",
    "    \"\"\"\n",
    "    rather than doing gradient descent, we do coordinate descent,\n",
    "    where we go in the steepest direction of one coordinate,\n",
    "    use the new location to calculate the other coordinate's steepest decline, etc.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    error = nonlinear_error(u, v)\n",
    "    while iteration < stop:\n",
    "        du = dE_du(u, v)\n",
    "        u -= eta * du\n",
    "        dv = dE_dv(u, v)\n",
    "        v -= eta * dv\n",
    "        error = nonlinear_error(u, v)\n",
    "        iteration += 1\n",
    "    return (error)\n",
    "\n",
    "u, v, iteration = gradient_descent()\n",
    "print(f'u: {u}')\n",
    "print(f'v: {v}')\n",
    "print(f'iterations: {iteration}')\n",
    "\n",
    "print()\n",
    "\n",
    "error = coordinate_descent()\n",
    "print(f'error for coordinate descent: {error}')\n",
    "\n",
    "#sample output\n",
    "# u: 0.04473629039778207\n",
    "# v: 0.023958714099141746\n",
    "# iterations: 10\n",
    "\n",
    "# error for coordinate descent: 0.13981379199615315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 100\n",
      "run 200\n",
      "run 300\n",
      "run 400\n",
      "run 500\n",
      "run 600\n",
      "run 700\n",
      "run 800\n",
      "run 900\n",
      "run 1000\n",
      "The average E_out is 0.11080479417685717\n",
      "The average epoch to get there is 374.83983983983984\n"
     ]
    }
   ],
   "source": [
    "#problems 8-9\n",
    "#we co-opt / modify some of the code we wrote for pset1/2:\n",
    "\n",
    "def generate_point(boundary1, boundary2, dimension):\n",
    "    \"\"\"\n",
    "    Generate random dimension-dimensional point on \n",
    "    [boundary1, boundary 2] X [boundary1, boundary2] X etc. space.\n",
    "    Returns ndarray of (x1, x2) point\n",
    "    \"\"\"\n",
    "    #set random number generator\n",
    "    rg = np.random.default_rng()\n",
    "    \n",
    "    random_point = np.zeros(dimension)\n",
    "    for i in range(dimension):\n",
    "        random_point[i] = rg.uniform(low = boundary1, high = boundary2, size = 1)\n",
    "    return random_point\n",
    "\n",
    "def generate_target_f_linear():\n",
    "    \"\"\"\n",
    "    Returns slope and intercept of line connecting two random points\n",
    "    \"\"\"\n",
    "    point_1 = generate_point(-1, 1, 2)\n",
    "    point_2 = generate_point(-1, 1, 2)\n",
    "    \n",
    "    # slope = (y2 - y1) / (x2 - x1)\n",
    "    slope = (point_2[1] - point_1[1]) / (point_2[0] - point_1[0])\n",
    "    # intercept = y1 - slope * x1\n",
    "    intercept = point_2[1] - slope * point_2[0]\n",
    "    return(slope, intercept)\n",
    "\n",
    "\n",
    "def classify_point_linear(random_point, slope, intercept):\n",
    "    \"\"\"\n",
    "    Given random_point in (x, y) form and a slope and intercept,\n",
    "    label the point\n",
    "    +1 if it falls above the line\n",
    "    -1 if it falls below the line\n",
    "    \"\"\"\n",
    "    if random_point[1] > slope * random_point[0] + intercept:\n",
    "        classification = 1\n",
    "    else:\n",
    "        classification = -1\n",
    "    return classification\n",
    "\n",
    "\n",
    "def create_data_linear(N, slope, intercept):\n",
    "    \"\"\"\n",
    "    Creates N points for testing data using target f(X) = slope * X + intercept;\n",
    "    Notes:\n",
    "        Target function f is a line connecting two points in X space [-1, 1] x [-1, 1]\n",
    "        Classification is based on whether points lie above or below f\n",
    "        \n",
    "    Inputs:\n",
    "        N (int)\n",
    "        slope (float), from generate_target_f_linear\n",
    "        intercept (float), from generate_target_f_linear\n",
    "        \n",
    "    Outputs:\n",
    "        X (ndarray), X.shape = (N, 3); x0 = 1\n",
    "        Y (ndarray), Y.shsape = (N, )\n",
    "    \"\"\"\n",
    "    # create matrix X, where x0 is always 1, to accomodate w0\n",
    "    X = np.ones((N, 3))\n",
    "    Y = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        random_point = generate_point(-1, 1, 2)\n",
    "        classification = classify_point_linear(random_point, slope, intercept)\n",
    "        X[i, 1:3] = random_point\n",
    "        Y[i] = classification\n",
    "        \n",
    "    return (X, Y)\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_error_gradient(x, y, weights):\n",
    "    \"\"\"\n",
    "    cross entropy gradient, slide 23 of lecture 9\n",
    "    for a single point at a time\n",
    "    \"\"\"\n",
    "    return - y * x/(1 + np.exp(y * np.dot(x, weights)))\n",
    "\n",
    "\n",
    "\n",
    "def update_weight(x, y, weights, eta):\n",
    "    \"\"\"\n",
    "    update weight according to a learning rate and cross_entropy_error\n",
    "    \"\"\"\n",
    "    weights -= eta * cross_entropy_error_gradient(x, y, weights)\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "def train_weights(slope, intercept, eta = .01, training_size = 100, stop = .01):\n",
    "    \"\"\"\n",
    "    Train with error measure as logistic regression's cross entropy error; \n",
    "    Inputs:\n",
    "        eta (learning rate)\n",
    "        training_size (size of training set)\n",
    "        stop (when subsequent weight adjustments are less than stop, return)\n",
    "    Outputs:\n",
    "        weights_2 (final weights based on given parameters)\n",
    "        epoch (how many epochs to get to weights_2)\n",
    "    \"\"\"\n",
    "    \n",
    "    #generate training data\n",
    "    X_training, Y_training = create_data_linear(training_size, slope, intercept)\n",
    "    \n",
    "    #initialize epoch counter and weights; keep two weights to allow comparisons\n",
    "    epoch = 0\n",
    "    weights_1 = np.ones(3)\n",
    "    weights_2 = np.zeros(3)\n",
    "    \n",
    "    #learn using sgd and logistic regression\n",
    "    while np.linalg.norm(weights_2 - weights_1) >= 0.01:\n",
    "        weights_1 = weights_2.copy()\n",
    "\n",
    "        #randomize data for presentation to stochastic g.d. with a random index\n",
    "        random_index = rg.choice(np.arange(training_size), size = training_size, replace = False)\n",
    "        \n",
    "        for index in random_index:\n",
    "            x_point, y_point = X_training[index], Y_training[index]\n",
    "            weights_2 = update_weight(x_point, y_point, weights_2, eta)\n",
    "\n",
    "        epoch += 1\n",
    "    \n",
    "    return (weights_2, epoch)\n",
    "\n",
    "\n",
    "\n",
    "def test_weights(slope, intercept, testing_size = 100, eta = .01, training_size = 100, stop = .01):\n",
    "    \"\"\"\n",
    "    determine E_out for weights determined by train_weights() function\n",
    "    testing on a separate set of data\n",
    "    Outputs:\n",
    "        cross_entropy_error_out (E_out)\n",
    "        epoch (epochs taken to train the model)\n",
    "    \"\"\"\n",
    "    \n",
    "    #generate testing data\n",
    "    X_testing, Y_testing = create_data_linear(testing_size, slope, intercept)\n",
    "    \n",
    "    #what does the trained model say?\n",
    "    weights, epoch= train_weights(\n",
    "        slope, intercept, eta = eta, training_size = training_size, stop = stop\n",
    "        )\n",
    "    \n",
    "    #calcalate test sample error\n",
    "    cross_entropy_error_out = 0\n",
    "    for i in range(testing_size):\n",
    "        #slide 16, lecture 9\n",
    "        cross_entropy_error_out += np.log(1 + np.exp(-Y_testing[i] * np.dot(weights, X_testing[i])))\n",
    "    \n",
    "    return (cross_entropy_error_out / testing_size, epoch)\n",
    "\n",
    "\n",
    "def experiment(runs = 1000, testing_size = 100, eta = .01, training_size = 100, stop = .01):\n",
    "    \"\"\"\n",
    "    Do problems 8/9 using parameters given, and helper functions above\n",
    "    \"\"\"\n",
    "    \n",
    "    slope, intercept = generate_target_f_linear()\n",
    "    \n",
    "    #initialize counters to sum up and average over the runs\n",
    "    epoch_counter = 0\n",
    "    e_out_counter = 0\n",
    "    \n",
    "    #\n",
    "    for run in range(runs):\n",
    "        if (run + 1) % 100 == 0:\n",
    "            print ('run', run + 1)\n",
    "        cross_entropy_error_out, epoch = test_weights(\n",
    "            slope, intercept, testing_size = testing_size, eta = eta, training_size = training_size, stop = stop\n",
    "            )\n",
    "        e_out_counter += cross_entropy_error_out\n",
    "        epoch_counter += epoch\n",
    "        \n",
    "    return e_out_counter / run, epoch_counter / run\n",
    "\n",
    "\n",
    "e_out, epoch = experiment()\n",
    "\n",
    "print(f\"The average E_out is {e_out}\")\n",
    "print(f\"The average epoch to get there is {epoch}\")\n",
    "\n",
    "#sample output\n",
    "# run 100\n",
    "# run 200\n",
    "# run 300\n",
    "# run 400\n",
    "# run 500\n",
    "# run 600\n",
    "# run 700\n",
    "# run 800\n",
    "# run 900\n",
    "# run 1000\n",
    "# The average E_out is 0.11080479417685717\n",
    "# The average epoch to get there is 374.83983983983984"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
