{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 156a Extra Credit\n",
    "#### Victoria Liu\n",
    "#### November 20, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't end up using cloud computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running from the command line, I decided to place the code in a Jupyter so I could better loop over the number of neurons I wanted to test for each layer; there might be a way to do this on the command line as well, but I'm just more familiar with Jupyter Notebooks (incidentally, this homework is typed on a Jupyter Notebook as well). I got the following accuracies / number of parameters for each of the values I tested. I put everything in a dataframe for ease of reading. In the interest of keeping the notebook tidy, in the future, I will take out most of the code related to the creation of the dataframe when converting the notebook to pdf. I just wanted to show it for the first problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9685</td>\n",
       "      <td>32685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>32940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>33195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>33450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9706</td>\n",
       "      <td>34327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>40785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>41090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>41395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>41700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>44835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9695</td>\n",
       "      <td>45165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>45495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>45825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9734</td>\n",
       "      <td>56985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>57390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>57795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>58200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9811</td>\n",
       "      <td>178110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>241410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>400</td>\n",
       "      <td>800</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>642810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer1  layer2  accuracy  parameters\n",
       "0       40      25    0.9685       32685\n",
       "1       40      30    0.9687       32940\n",
       "2       40      35    0.9699       33195\n",
       "3       40      40    0.9699       33450\n",
       "19      41      41    0.9706       34327\n",
       "4       50      25    0.9688       40785\n",
       "5       50      30    0.9705       41090\n",
       "6       50      35    0.9732       41395\n",
       "7       50      40    0.9718       41700\n",
       "8       55      25    0.9715       44835\n",
       "9       55      30    0.9695       45165\n",
       "10      55      35    0.9704       45495\n",
       "11      55      40    0.9732       45825\n",
       "12      70      25    0.9734       56985\n",
       "13      70      30    0.9753       57390\n",
       "14      70      35    0.9752       57795\n",
       "15      70      40    0.9742       58200\n",
       "18     200     100    0.9811      178110\n",
       "16     200     400    0.9819      241410\n",
       "17     400     800    0.9815      642810"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "layer1 = [40, 40, 40, 40, 50, 50, 50, 50, 55, 55, 55, 55, 70, 70, 70, 70, 200, 400, 200, 41]\n",
    "layer2 = [25, 30, 35, 40, 25, 30, 35, 40, 25, 30, 35, 40, 25, 30, 35, 40, 400, 800, 100, 41]\n",
    "accuracy = [0.9685, 0.9687, 0.9699, 0.9699, 0.9688, 0.9705, 0.9732, 0.9718, 0.9715, 0.9695,\n",
    "            0.9704, 0.9732, 0.9734, 0.9753, 0.9752, 0.9742, 0.9819, 0.9815, 0.9811, 0.9706]\n",
    "parameters = [32685, 32940, 33195, 33450, 40785, 41090, 41395, 41700, 44835, 45165, 45495, \n",
    "              45825, 56985, 57390, 57795, 58200, 241410, 642810, 178110, 34327]\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "    'layer1' : layer1,\n",
    "    'layer2' : layer2,\n",
    "    'accuracy' : accuracy,\n",
    "    'parameters' : parameters\n",
    "    }\n",
    "    )\n",
    "df = df.sort_values(by=['parameters'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the learning curves. The models are titled as following: \"dense_2layers_*layer1*_*layer2*_learn.jpg\"\n",
    "\n",
    "<img src=\"dense_2layers_40_25_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_40_30_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_40_35_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_40_40_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_50_25_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_50_30_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_50_35_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_50_40_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_55_25_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_55_30_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_55_35_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_55_40_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_70_25_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_70_30_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_70_35_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_70_40_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_200_400_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_400_800_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_41_41_learn.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves are as expected--as we feed in more epochs, the training and validation accuracy both get better, but at one point the training accuracy gets better because the model may be slightly over-fitting. Luckily, the overfitting doesn't seem to be a huge issue, since the validation and training errors are still close for most of the parameters. I noticed that for smaller numbers of parameters, the difference between training / validation errors is less, which makes sense wrt to the VC dimension. Interestingly, even when we have $642810$ parameters, although the difference between training and validation is quite great, the validation accuracy is still the highest out of all the models. However, it seems like no matter how many neurons we add, we can never seem to get $100$% accuracy, which is as expected.\n",
    "\n",
    "\n",
    "Overall, it seems like the more neurons we add to either layer, the better the accuracy gets. For example, if we keep the number of neurons in the first layer constant and add more neurons in the second layer, the validation accuracy will always increase, and vice versa for keeping the second layer constant. Interestingly, it seems like it might be a good idea to keep the number of neurons in the first and second layers similar; otherwise, the number of parameters will skyrocket. For example, when we have $41$ neurons in both layers, we get $0.9706$ accuracy and $34327$ parameters. When we have a comparable number of total neurons but distributed in $50$ vs $30$, we get $0.9705$ accuracy but $41090$ parameters. Also, the smallest number of parameters for which I could get over $97$ accuracy was around $34327$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out a few regularizer strengths and put them in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regularizer</th>\n",
       "      <th>validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.9811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.9824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.9784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.9798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.9806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.9784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.9792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.000000e-03</td>\n",
       "      <td>0.9626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.9179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regularizer  validation accuracy\n",
       "7  0.000000e+00               0.9811\n",
       "6  1.000000e-10               0.9824\n",
       "2  1.000000e-08               0.9784\n",
       "1  1.000000e-06               0.9798\n",
       "3  1.000000e-05               0.9806\n",
       "4  1.000000e-04               0.9784\n",
       "8  1.000000e-03               0.9792\n",
       "9  3.000000e-03               0.9626\n",
       "5  1.000000e-02               0.9179\n",
       "0  1.000000e+00               0.1135"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularizers = [1, 0.000001, 0.00000001, 0.00001, 0.0001, 0.01, 0.0000000001, 0, 0.001, 0.003]\n",
    "validation_accuracy = [0.1135, 0.9798, 0.9784, 0.9806, 0.9784, 0.9179, 0.9824, 0.9811, 0.9792, 0.9626]\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'regularizer' : regularizers,\n",
    "        'validation accuracy' : validation_accuracy\n",
    "    }\n",
    "    )\n",
    "df = df.sort_values(by=['regularizer'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be too much issue with overfitting, since no regularizer actually did almost just as well as having a regularizer. However, there is clearly an underfitting issue for $\\lambda > 0.003$, since we see a drastic fall in validation accuracy there. It seems like having $\\lambda = 0.003$ gives around $96%$ to $97%$ accuracy, similar to what we saw in the smaller neural nets.\n",
    "\n",
    "The learning curves are as follows. As expected, the training and validation curves get further apart as the regularizer decreases, because we are fitting more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dense_reg_0_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_2_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_3_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_4_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_5_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_6_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_8_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_10_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_0.003_learn.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the activation $\\theta = \\tanh(s)$. From class, we saw that $\\frac{d\\theta}{ds} = 1 - \\theta^{2}(s)$. As $|s| \\rightarrow \\infty$, $\\theta^{2} \\rightarrow 1$, and $\\frac{d\\theta}{ds} \\rightarrow 0$. This is bad news for our learning, since:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_{i}^{(l - 1)} = \\sum^{d^{(l)}}_{j=1} \\delta^{(l)}_{j} \\cdot w_{ij}^{(l)} \\cdot \\frac{d\\theta}{ds}\n",
    "\\end{align}\n",
    "\n",
    "If our last term goes to zero, then our delta's could get very small, and weights will get updated very slowly, since:\n",
    "\n",
    "\\begin{align}\n",
    "w^{(l)}_{ij} = w^{(l)}_{ij} - \\eta x^{(l-1)}_{i} \\delta_{j}^{(l)} \n",
    "\\end{align}\n",
    "\n",
    "I could see this being an issue for the rate of convergence when using the $\\tanh$ function. On the other hand, ReLu wouldn't be as much of a problem because as long as $s$ is positive, the slope would always be $1$, so we wouldn't see $delta$ continuously getting smaller for large values of $s$. I'm assuming that for negative values of $s$, even though $\\frac{d\\theta}{ds}$ goes to $0$, the positives values of $s$ more than make up for it, so there is much faster convergence in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just play around with the number of layers and the architecture of the layers, and we keep the same regularizer type, regularization parameter (0.00001), and nonlinearity ReLu. When we order our dataframe by validation accuracy, we see that smaller neural networks seem to do better. In fact, our highest accuracy, at $.98$, was when we had a single layer with all $200$ neurons in that layer. It also seems that we do better when the first layer has more neurons than the second layer. For example, when we have $150$ in the first layer and $50$ in the second, we get $.9778$ accuracy; when the layer counts are flipped, we get $.9754$. This might be due to a bottleneck effect, where not having enough neurons in the first layer prevented enough useful information from passing into the second layer. We can somewhat verify this by seeing how badly we did when we had fewer than $40$ neurons in the first layer; the highest we got was $0.9677$, and when we only had $10$ neurons in the first layer, we got. $.9512$ accuracy. Now, let's take the four top performers and vary the other parameters to see how they affect the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>layers</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>architecture</th>\n",
       "      <th>regularization_type</th>\n",
       "      <th>reg_param</th>\n",
       "      <th>nonlinearity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9512</td>\n",
       "      <td>4</td>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 60, 60, 70]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.9549</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>[20, 20, 20, 20, 20, 20, 20, 20, 20, 20]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9665</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[25, 25, 150]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9677</td>\n",
       "      <td>5</td>\n",
       "      <td>adam</td>\n",
       "      <td>[40, 40, 40, 40, 40]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9725</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[50, 100, 50]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9733</td>\n",
       "      <td>4</td>\n",
       "      <td>adam</td>\n",
       "      <td>[50, 50, 50, 50]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9735</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[70, 70, 60]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9745</td>\n",
       "      <td>4</td>\n",
       "      <td>adam</td>\n",
       "      <td>[70, 60, 60, 10]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9754</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>[50, 150]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9761</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[150, 25, 25]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9763</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9772</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9778</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9801</td>\n",
       "      <td>1</td>\n",
       "      <td>adam</td>\n",
       "      <td>[200]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_accuracy  layers optimizer                              architecture  \\\n",
       "9         0.9512       4      adam                          [10, 60, 60, 70]   \n",
       "11        0.9549      10      adam  [20, 20, 20, 20, 20, 20, 20, 20, 20, 20]   \n",
       "5         0.9665       3      adam                             [25, 25, 150]   \n",
       "10        0.9677       5      adam                      [40, 40, 40, 40, 40]   \n",
       "4         0.9725       3      adam                             [50, 100, 50]   \n",
       "8         0.9733       4      adam                          [50, 50, 50, 50]   \n",
       "7         0.9735       3      adam                              [70, 70, 60]   \n",
       "13        0.9745       4      adam                          [70, 60, 60, 10]   \n",
       "2         0.9754       2      adam                                 [50, 150]   \n",
       "6         0.9761       3      adam                             [150, 25, 25]   \n",
       "1         0.9763       2      adam                                [100, 100]   \n",
       "3         0.9772       3      adam                             [100, 50, 50]   \n",
       "12        0.9778       2      adam                                 [150, 50]   \n",
       "0         0.9801       1      adam                                     [200]   \n",
       "\n",
       "   regularization_type  reg_param nonlinearity  \n",
       "9                   l2    0.00001         relu  \n",
       "11                  l2    0.00001         relu  \n",
       "5                   l2    0.00001         relu  \n",
       "10                  l2    0.00001         relu  \n",
       "4                   l2    0.00001         relu  \n",
       "8                   l2    0.00001         relu  \n",
       "7                   l2    0.00001         relu  \n",
       "13                  l2    0.00001         relu  \n",
       "2                   l2    0.00001         relu  \n",
       "6                   l2    0.00001         relu  \n",
       "1                   l2    0.00001         relu  \n",
       "3                   l2    0.00001         relu  \n",
       "12                  l2    0.00001         relu  \n",
       "0                   l2    0.00001         relu  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architectures = [\n",
    "    [200],\n",
    "    [100, 100],\n",
    "    [50, 150],\n",
    "    [100, 50, 50],\n",
    "    [50, 100, 50],\n",
    "    [25, 25, 150],\n",
    "    [150, 25, 25],\n",
    "    [70, 70, 60],\n",
    "    [50, 50, 50, 50],\n",
    "    [10, 60, 60, 70],\n",
    "    [40, 40, 40, 40, 40],\n",
    "    [20, 20, 20, 20, 20, 20, 20, 20, 20, 20,],\n",
    "    [150, 50],\n",
    "    [70, 60, 60, 10],\n",
    "]\n",
    "\n",
    "val_accuracies = [0.9800999760627747,\n",
    "     0.9763000011444092,\n",
    "     0.9753999710083008,\n",
    "     0.9771999716758728,\n",
    "     0.9725000262260437,\n",
    "     0.9664999842643738,\n",
    "     0.9761000275611877,\n",
    "     0.9735000133514404,\n",
    "     0.9732999801635742,\n",
    "     0.951200008392334,\n",
    "     0.9677000045776367,\n",
    "     0.9549000263214111,\n",
    "     0.9778000116348267,\n",
    "     0.9745000004768372,\n",
    "    ]\n",
    "\n",
    "layers = [len(architecture) for architecture in architectures\n",
    "         ]\n",
    "optimizers = ['adam'] * 14\n",
    "\n",
    "reg_types = ['l2'] * 14\n",
    "\n",
    "reg_params = [0.00001] * 14\n",
    "\n",
    "nonlinearities = ['relu'] * 14\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'val_accuracy' : val_accuracies,\n",
    "    'layers' : layers,\n",
    "    'optimizer' : optimizers,\n",
    "    'architecture' : architectures,\n",
    "    'regularization_type' : reg_types,\n",
    "    'reg_param' : reg_params,\n",
    "    'nonlinearity' : nonlinearities,\n",
    "})\n",
    "\n",
    "df = df.sort_values(by='val_accuracy')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>layers</th>\n",
       "      <th>architecture</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>regularization_type</th>\n",
       "      <th>reg_param</th>\n",
       "      <th>nonlinearity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0974</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>sgd</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>sgd</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>sgd</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.4877</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>sgd</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9705</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9722</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9732</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9743</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9763</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9772</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9778</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9801</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_accuracy  layers   architecture optimizer regularization_type  \\\n",
       "15        0.0974       1          [200]       sgd                  l2   \n",
       "9         0.0980       3  [100, 50, 50]      adam                  l2   \n",
       "4         0.0980       2     [100, 100]      adam                  l1   \n",
       "5         0.0980       3  [100, 50, 50]      adam                  l1   \n",
       "6         0.0980       2      [150, 50]      adam                  l1   \n",
       "7         0.0980       1          [200]      adam                  l1   \n",
       "8         0.0980       2     [100, 100]      adam                  l2   \n",
       "14        0.0980       2      [150, 50]       sgd                  l2   \n",
       "10        0.0980       2      [150, 50]      adam                  l2   \n",
       "11        0.0980       1          [200]      adam                  l2   \n",
       "12        0.0980       2     [100, 100]       sgd                  l2   \n",
       "13        0.4877       3  [100, 50, 50]       sgd                  l2   \n",
       "1         0.9705       3  [100, 50, 50]      adam                  l2   \n",
       "3         0.9722       1          [200]      adam                  l2   \n",
       "0         0.9732       2     [100, 100]      adam                  l2   \n",
       "2         0.9743       2      [150, 50]      adam                  l2   \n",
       "1         0.9763       2     [100, 100]      adam                  l2   \n",
       "3         0.9772       3  [100, 50, 50]      adam                  l2   \n",
       "12        0.9778       2      [150, 50]      adam                  l2   \n",
       "0         0.9801       1          [200]      adam                  l2   \n",
       "\n",
       "    reg_param nonlinearity  \n",
       "15    0.00001         relu  \n",
       "9     0.00100         relu  \n",
       "4     0.00001         relu  \n",
       "5     0.00001         relu  \n",
       "6     0.00001         relu  \n",
       "7     0.00001         relu  \n",
       "8     0.00100         relu  \n",
       "14    0.00001         relu  \n",
       "10    0.00100         relu  \n",
       "11    0.00100         relu  \n",
       "12    0.00001         relu  \n",
       "13    0.00001         relu  \n",
       "1     0.00001      sigmoid  \n",
       "3     0.00001      sigmoid  \n",
       "0     0.00001      sigmoid  \n",
       "2     0.00001      sigmoid  \n",
       "1     0.00001         relu  \n",
       "3     0.00001         relu  \n",
       "12    0.00001         relu  \n",
       "0     0.00001         relu  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architectures = [\n",
    "    [100, 100],\n",
    "    [100, 50, 50],\n",
    "    [150, 50],\n",
    "    [200],\n",
    "    [100, 100],\n",
    "    [100, 50, 50],\n",
    "    [150, 50],\n",
    "    [200],\n",
    "    [100, 100],\n",
    "    [100, 50, 50],\n",
    "    [150, 50],\n",
    "    [200],\n",
    "    [100, 100],\n",
    "    [100, 50, 50],\n",
    "    [150, 50],\n",
    "    [200],\n",
    "]\n",
    "\n",
    "val_accuracies = [\n",
    "     0.9732000231742859,\n",
    "     0.9704999923706055,\n",
    "     0.9743000268936157,\n",
    "     0.9721999764442444,\n",
    "     0.09799999743700027,\n",
    "     0.09799999743700027,\n",
    "     0.09799999743700027,\n",
    "     0.09799999743700027,\n",
    "     0.09799999743700027,\n",
    "     0.09799999743700027,\n",
    "     0.09799999743700027,\n",
    "     0.09799999743700027,\n",
    "     0.09799999743700027,\n",
    "     0.4876999855041504,\n",
    "     0.09799999743700027,\n",
    "     0.09740000218153,\n",
    "    ]\n",
    "\n",
    "layers = [len(architecture) for architecture in architectures\n",
    "         ]\n",
    "\n",
    "optimizers = ['adam'] * 12 + ['sgd'] * 4\n",
    "\n",
    "reg_types = ['l2'] * 4 + ['l1'] * 4 + ['l2'] * 8\n",
    "\n",
    "reg_params = [0.00001] * 8 + [0.001] * 4 + [0.00001] * 4\n",
    "\n",
    "nonlinearities = ['sigmoid'] * 4 + ['relu'] * 12 \n",
    "\n",
    "import pandas as pd\n",
    "df_2 = pd.DataFrame({\n",
    "    'val_accuracy' : val_accuracies,\n",
    "    'layers' : layers,\n",
    "    'architecture' : architectures,\n",
    "    'optimizer' : optimizers,\n",
    "    'regularization_type' : reg_types,\n",
    "    'reg_param' : reg_params,\n",
    "    'nonlinearity' : nonlinearities,\n",
    "})\n",
    "\n",
    "\n",
    "df_4 = df.loc[(df.index == 0) | (df.index == 1) | (df.index == 3) | (df.index == 12)]\n",
    "\n",
    "df_2\n",
    "df_2 = df_2.append(df_4)\n",
    "df_2 = df_2.sort_values(by = ['val_accuracy'])\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is again ordered by validation accuracy. Right off the bat, we can see that `l1` is a bad regularization method when the regularizer is $0.00001$, not even getting $10$% accuracy, which is pretty much just randomly guessing at that point. I guess I shouldn't be too surprised, but we get similarly low accuracies for `l2` when the regularizer is $0.001$, and the accuracies are the same as the accuracies for `l1` at regularizer strength $0.00001$. This is probably because `l1` is kind of like the square root of `l2`, and $0.001$ is the square root of $0.00001$ (not sure if this logic completely holds, but it made sense in my head...). In addition, we see that `sgd` actually does a pretty bad job compared to `adam`, with `sgd` failing to get above $50$% accuracy. Lastly, `ReLu` appears to out-perform `Sigmoid` activation at high levels of accuracy. This suggests that the difference between choosing `ReLu` vs `Sigmoid` may not cause as drastic a difference as choosing bad values for other parameters, but there is still a difference.\n",
    "\n",
    "\n",
    "When looking at the learning curves, I couldn't find any \"pattern\" for when the training vs validation errors track close together and when they track further apart. I did notice that the learning curve for our best contender (one layer with $200$ neurons, adam, l2, $0.00001$ regularizer strength, ReLu, $0.9801$ accuracy) had a pretty high training accuracy at almost $100$%, but the validation accuracy didn't track as closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dense_2layers_50-150_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_adam_relu_l1_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_sgd_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_150-25-25_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_adam_sigmoid_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_5layers_40-40-40-40-40_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_adam_relu_l1_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_4layers_50-50-50-50_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_adam_relu_.001_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_adam_sigmoid_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_adam_relu_.001_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_50-100-50_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_10layers_20-20-20-20-20-20-20-20-20-20_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_25-25-150_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_4layers_70-60-60-10_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_sgd_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_adam_sigmoid_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_adam_relu_l1_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_sgd_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_adam_sigmoid_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_70-70-60_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_adam_relu_l1_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_sgd_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_adam_relu_.001_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_4layers_10-60-60-70_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_adam_relu_.001_learn.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the model summaries, the densely connected model uses $178,110$ paramteres to train, while the convolutional model uses just $7,240$ parameters. That is more than $25$ orders of magnitude in difference and great news for our VC dimension / generalizability according to Hoeffding. Here are the learning curves. We can see that our intuition was correct--the learning curves for the convolutional neural network track better after $10$ epochs than the learning curves for the densely connected network. The validation accuracy is $0.9874$, while the training accuracy is $0.9877$. Not only do the accuracies track better with out of sample, but the accuracy itself is also higher than the accuracy from the densely connected network. Earlier in problem 2, the absolute best validation accuracy we could get was $0.9815$, so we can conclusively say that in this particular learning scenario, convolutional neural networks worked better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dense_no_reg_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"conv_vs_dense_learn.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n",
    "\n",
    "#### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L^{(D)}}{\\partial D(X)} &= \\frac{\\partial}{\\partial D(X)} \\left[ -\\frac{1}{2} \\mathbb{E}_{x\\sim p_{data}} \\left[ \\log{D(x)} \\right] - \\frac{1}{2} \\mathbb{E}_{x \\sim p_{model}} \\left[ 1 - \\log{D(x)} \\right] \\right] \\\\[1em]\n",
    "\\frac{\\partial L^{(D)}}{\\partial D(X)} &= -\\frac{1}{2} \\frac{\\partial}{\\partial D(X)} \\left[  \\mathbb{E}_{x\\sim p_{data}} \\left[ \\log{D(x)} \\right] \\right] - \\frac{1}{2} \\frac{\\partial}{\\partial D(X)} \\left[ \\mathbb{E}_{x \\sim p_{model}} \\left[ 1 - \\log{D(x)} \\right] \\right] \\\\[1em]\n",
    "\\frac{\\partial L^{(D)}}{\\partial D(X)} &= -\\frac{1}{2} \\int \\frac{1}{D(x)} p_{data}(x) + \\frac{1}{2} \\int \\frac{1}{1 - D(x)} p_{model}(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{2} \\int \\frac{1}{D(x)} p_{data}(x) &= \\frac{1}{2} \\int \\frac{1}{1 - D(x)} p_{model}(x) \\\\[1em]\n",
    "\\frac{p_{data}(x)}{D(x)} &= \\frac{p_{model}(x)}{1 - D(x)} \\\\[1em]\n",
    "p_{data}(x) - p_{data}(x) \\cdot D(x) &= p_{model}(x) \\cdot D(x) \\\\[1em]\n",
    "D(x) &= \\frac{p_{data}(x)}{p_{model}(x) + p_{data}(x)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3\n",
    "I do indeed get that solution. I think this makes sense, because the discriminator is trying to make the loss as large as possible, and it can do so by quantifying the \"probability\" of getting the data probability distribution, compared to all the inputs it could get, from either the model probability distribution or the data probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8\n",
    "\n",
    "It looks like we are alternatively training the generator and then the discriminator. The generator is trying to fool the discriminator, and the discriminator is trying to point out which images are generated and which images are real. Looking at the code, it seems like we are training them separately (i.e. not at the same time), and this seems to be implemented using `discriminator.trainable = False` vs `discriminator.trainable = True`. In other words, the discriminator weights are 'frozen' in the adversarial net, and only the generator weights are trained. At other times, we will do the opposite to train the discriminator. Let's extract all the loss values for the generator and the discriminator and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generator_loss</th>\n",
       "      <th>discriminator_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0810</td>\n",
       "      <td>0.4177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.8224</td>\n",
       "      <td>0.4644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5916</td>\n",
       "      <td>0.5411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3012</td>\n",
       "      <td>0.5586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5790</td>\n",
       "      <td>0.5100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.4779</td>\n",
       "      <td>0.5152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.3847</td>\n",
       "      <td>0.5257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.3238</td>\n",
       "      <td>0.5357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.3393</td>\n",
       "      <td>0.5537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.2527</td>\n",
       "      <td>0.5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.2043</td>\n",
       "      <td>0.5874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2488</td>\n",
       "      <td>0.5770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.1574</td>\n",
       "      <td>0.5916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.1015</td>\n",
       "      <td>0.5935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.1394</td>\n",
       "      <td>0.5964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0812</td>\n",
       "      <td>0.6058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0605</td>\n",
       "      <td>0.6108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0493</td>\n",
       "      <td>0.6177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0400</td>\n",
       "      <td>0.6150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0527</td>\n",
       "      <td>0.6205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0911</td>\n",
       "      <td>0.6178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.9498</td>\n",
       "      <td>0.6342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.9671</td>\n",
       "      <td>0.6284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9533</td>\n",
       "      <td>0.6339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.6259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.6341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.6350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.6449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9662</td>\n",
       "      <td>0.6344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.9488</td>\n",
       "      <td>0.6390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.9827</td>\n",
       "      <td>0.6318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.9534</td>\n",
       "      <td>0.6426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.9723</td>\n",
       "      <td>0.6394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.9265</td>\n",
       "      <td>0.6419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.6403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.9239</td>\n",
       "      <td>0.6471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.6443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.9454</td>\n",
       "      <td>0.6492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.6530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.9238</td>\n",
       "      <td>0.6555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.6561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.8565</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.8733</td>\n",
       "      <td>0.6480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.8896</td>\n",
       "      <td>0.6481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.6467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.8915</td>\n",
       "      <td>0.6592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.6542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.8722</td>\n",
       "      <td>0.6517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.9235</td>\n",
       "      <td>0.6509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.6639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.8833</td>\n",
       "      <td>0.6557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.8528</td>\n",
       "      <td>0.6618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.6627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.8559</td>\n",
       "      <td>0.6567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.8373</td>\n",
       "      <td>0.6621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.8586</td>\n",
       "      <td>0.6563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.8868</td>\n",
       "      <td>0.6616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.8635</td>\n",
       "      <td>0.6579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.8374</td>\n",
       "      <td>0.6684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.6617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.8482</td>\n",
       "      <td>0.6591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.8966</td>\n",
       "      <td>0.6543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.8733</td>\n",
       "      <td>0.6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.6704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.8194</td>\n",
       "      <td>0.6691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.8325</td>\n",
       "      <td>0.6659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.8166</td>\n",
       "      <td>0.6719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.8374</td>\n",
       "      <td>0.6608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.8344</td>\n",
       "      <td>0.6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.6653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.8260</td>\n",
       "      <td>0.6630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.6576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.6610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.8666</td>\n",
       "      <td>0.6588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.6627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.8661</td>\n",
       "      <td>0.6657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.7947</td>\n",
       "      <td>0.6737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.8235</td>\n",
       "      <td>0.6670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.6629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.8259</td>\n",
       "      <td>0.6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.6637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.8252</td>\n",
       "      <td>0.6637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.8356</td>\n",
       "      <td>0.6585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.8555</td>\n",
       "      <td>0.6656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.8195</td>\n",
       "      <td>0.6689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.8353</td>\n",
       "      <td>0.6668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.6675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.8213</td>\n",
       "      <td>0.6693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.8194</td>\n",
       "      <td>0.6664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.8231</td>\n",
       "      <td>0.6643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.8464</td>\n",
       "      <td>0.6639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.8166</td>\n",
       "      <td>0.6653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.6624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.8448</td>\n",
       "      <td>0.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.8528</td>\n",
       "      <td>0.6635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.8435</td>\n",
       "      <td>0.6633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.8270</td>\n",
       "      <td>0.6645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.6657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.8505</td>\n",
       "      <td>0.6601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.8272</td>\n",
       "      <td>0.6634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.8399</td>\n",
       "      <td>0.6641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.8186</td>\n",
       "      <td>0.6655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.8152</td>\n",
       "      <td>0.6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.8354</td>\n",
       "      <td>0.6660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.8355</td>\n",
       "      <td>0.6664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.8573</td>\n",
       "      <td>0.6682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.8717</td>\n",
       "      <td>0.6647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.8279</td>\n",
       "      <td>0.6642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.6671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.8317</td>\n",
       "      <td>0.6618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.8236</td>\n",
       "      <td>0.6671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.8322</td>\n",
       "      <td>0.6640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.6675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.8439</td>\n",
       "      <td>0.6604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.8337</td>\n",
       "      <td>0.6614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.6626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.8365</td>\n",
       "      <td>0.6638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.6658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.8266</td>\n",
       "      <td>0.6630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.8249</td>\n",
       "      <td>0.6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.8327</td>\n",
       "      <td>0.6663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.8582</td>\n",
       "      <td>0.6588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.8273</td>\n",
       "      <td>0.6637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.8358</td>\n",
       "      <td>0.6625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.8462</td>\n",
       "      <td>0.6589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.8306</td>\n",
       "      <td>0.6626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.8290</td>\n",
       "      <td>0.6645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.8374</td>\n",
       "      <td>0.6642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.8422</td>\n",
       "      <td>0.6637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.8427</td>\n",
       "      <td>0.6599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.8665</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.8274</td>\n",
       "      <td>0.6688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.8109</td>\n",
       "      <td>0.6666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.8281</td>\n",
       "      <td>0.6645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.8408</td>\n",
       "      <td>0.6638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.8312</td>\n",
       "      <td>0.6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.8317</td>\n",
       "      <td>0.6632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.6682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.8224</td>\n",
       "      <td>0.6642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.8099</td>\n",
       "      <td>0.6658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.8249</td>\n",
       "      <td>0.6640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.8222</td>\n",
       "      <td>0.6669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.8312</td>\n",
       "      <td>0.6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.8170</td>\n",
       "      <td>0.6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.8262</td>\n",
       "      <td>0.6657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.8176</td>\n",
       "      <td>0.6669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.8389</td>\n",
       "      <td>0.6666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.8281</td>\n",
       "      <td>0.6659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.8210</td>\n",
       "      <td>0.6699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.8134</td>\n",
       "      <td>0.6650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.8290</td>\n",
       "      <td>0.6660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.8157</td>\n",
       "      <td>0.6687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.8277</td>\n",
       "      <td>0.6679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.6654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.8194</td>\n",
       "      <td>0.6672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.8164</td>\n",
       "      <td>0.6666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.8344</td>\n",
       "      <td>0.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.8275</td>\n",
       "      <td>0.6747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.8078</td>\n",
       "      <td>0.6700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.8128</td>\n",
       "      <td>0.6718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.8246</td>\n",
       "      <td>0.6644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.8404</td>\n",
       "      <td>0.6677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.8093</td>\n",
       "      <td>0.6683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.6685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.8035</td>\n",
       "      <td>0.6682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.8145</td>\n",
       "      <td>0.6681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.8302</td>\n",
       "      <td>0.6639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.8128</td>\n",
       "      <td>0.6686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.8130</td>\n",
       "      <td>0.6672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.6675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.8272</td>\n",
       "      <td>0.6664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.8422</td>\n",
       "      <td>0.6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.6659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.8167</td>\n",
       "      <td>0.6671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.8129</td>\n",
       "      <td>0.6700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.8415</td>\n",
       "      <td>0.6629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.8222</td>\n",
       "      <td>0.6668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.8176</td>\n",
       "      <td>0.6673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.8182</td>\n",
       "      <td>0.6668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.8162</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.6643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.8156</td>\n",
       "      <td>0.6672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.8151</td>\n",
       "      <td>0.6687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.8037</td>\n",
       "      <td>0.6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.6684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.8083</td>\n",
       "      <td>0.6684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.8156</td>\n",
       "      <td>0.6690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.8029</td>\n",
       "      <td>0.6706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.8257</td>\n",
       "      <td>0.6670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.8201</td>\n",
       "      <td>0.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.8338</td>\n",
       "      <td>0.6666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.8239</td>\n",
       "      <td>0.6707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.6714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.8140</td>\n",
       "      <td>0.6738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.7896</td>\n",
       "      <td>0.6737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.8036</td>\n",
       "      <td>0.6723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.7995</td>\n",
       "      <td>0.6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.8114</td>\n",
       "      <td>0.6722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.7965</td>\n",
       "      <td>0.6743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.7932</td>\n",
       "      <td>0.6689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.8036</td>\n",
       "      <td>0.6712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.7949</td>\n",
       "      <td>0.6687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.8050</td>\n",
       "      <td>0.6690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.6724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.7976</td>\n",
       "      <td>0.6726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.8057</td>\n",
       "      <td>0.6695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.8073</td>\n",
       "      <td>0.6706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.6703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.6744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.8007</td>\n",
       "      <td>0.6726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.6704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.8029</td>\n",
       "      <td>0.6704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.8023</td>\n",
       "      <td>0.6705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.7999</td>\n",
       "      <td>0.6700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.8027</td>\n",
       "      <td>0.6706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.8124</td>\n",
       "      <td>0.6695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.8173</td>\n",
       "      <td>0.6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.7916</td>\n",
       "      <td>0.6762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.8025</td>\n",
       "      <td>0.6740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.7883</td>\n",
       "      <td>0.6748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.6694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.7977</td>\n",
       "      <td>0.6704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.8047</td>\n",
       "      <td>0.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.8006</td>\n",
       "      <td>0.6705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.8030</td>\n",
       "      <td>0.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.8034</td>\n",
       "      <td>0.6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.8052</td>\n",
       "      <td>0.6700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.8087</td>\n",
       "      <td>0.6716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.8187</td>\n",
       "      <td>0.6714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.7971</td>\n",
       "      <td>0.6741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.6773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.7868</td>\n",
       "      <td>0.6737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.7979</td>\n",
       "      <td>0.6761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.7883</td>\n",
       "      <td>0.6738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.6727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.8002</td>\n",
       "      <td>0.6735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.8012</td>\n",
       "      <td>0.6712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.6713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.6746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.8048</td>\n",
       "      <td>0.6722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.7924</td>\n",
       "      <td>0.6725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.7962</td>\n",
       "      <td>0.6754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.6754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.7999</td>\n",
       "      <td>0.6723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.6740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.7954</td>\n",
       "      <td>0.6723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.7925</td>\n",
       "      <td>0.6723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.7834</td>\n",
       "      <td>0.6747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.7899</td>\n",
       "      <td>0.6720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.8004</td>\n",
       "      <td>0.6748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.6727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.6769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.7940</td>\n",
       "      <td>0.6747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.6769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.6768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.7891</td>\n",
       "      <td>0.6722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.7867</td>\n",
       "      <td>0.6750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.7914</td>\n",
       "      <td>0.6746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.6770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.6770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.7888</td>\n",
       "      <td>0.6755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.7821</td>\n",
       "      <td>0.6760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.7837</td>\n",
       "      <td>0.6771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.7847</td>\n",
       "      <td>0.6734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.7908</td>\n",
       "      <td>0.6765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.7936</td>\n",
       "      <td>0.6767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.6766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.7880</td>\n",
       "      <td>0.6760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.7918</td>\n",
       "      <td>0.6761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.7849</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.7887</td>\n",
       "      <td>0.6756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.7912</td>\n",
       "      <td>0.6761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.7904</td>\n",
       "      <td>0.6736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.7953</td>\n",
       "      <td>0.6776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.7849</td>\n",
       "      <td>0.6789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.8070</td>\n",
       "      <td>0.6746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.6793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.7729</td>\n",
       "      <td>0.6786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.7711</td>\n",
       "      <td>0.6765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.7821</td>\n",
       "      <td>0.6750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.7977</td>\n",
       "      <td>0.6744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.7841</td>\n",
       "      <td>0.6746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.7809</td>\n",
       "      <td>0.6778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.7999</td>\n",
       "      <td>0.6775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.7835</td>\n",
       "      <td>0.6795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.7583</td>\n",
       "      <td>0.6776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.7792</td>\n",
       "      <td>0.6742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.6762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.6739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.7764</td>\n",
       "      <td>0.6753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.6737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.7827</td>\n",
       "      <td>0.6746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.6765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.6747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.7800</td>\n",
       "      <td>0.6748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.6784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.7972</td>\n",
       "      <td>0.6735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.7925</td>\n",
       "      <td>0.6776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.7809</td>\n",
       "      <td>0.6807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.7821</td>\n",
       "      <td>0.6769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.7701</td>\n",
       "      <td>0.6768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.7745</td>\n",
       "      <td>0.6781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.6751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.7978</td>\n",
       "      <td>0.6783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.7928</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.7845</td>\n",
       "      <td>0.6792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.6766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.7897</td>\n",
       "      <td>0.6757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.7870</td>\n",
       "      <td>0.6796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.7816</td>\n",
       "      <td>0.6774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.6776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.7769</td>\n",
       "      <td>0.6766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.6777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.6765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.7773</td>\n",
       "      <td>0.6776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.6756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.7793</td>\n",
       "      <td>0.6777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.6781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.7765</td>\n",
       "      <td>0.6774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.7949</td>\n",
       "      <td>0.6740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.6782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.7905</td>\n",
       "      <td>0.6743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.7701</td>\n",
       "      <td>0.6762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.7782</td>\n",
       "      <td>0.6762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>0.7831</td>\n",
       "      <td>0.6757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.7780</td>\n",
       "      <td>0.6773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0.7931</td>\n",
       "      <td>0.6780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.7788</td>\n",
       "      <td>0.6806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.7705</td>\n",
       "      <td>0.6784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.7673</td>\n",
       "      <td>0.6773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.6760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.6741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.7887</td>\n",
       "      <td>0.6759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.6785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.7802</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.6810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.7630</td>\n",
       "      <td>0.6782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.7781</td>\n",
       "      <td>0.6797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.6789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.7848</td>\n",
       "      <td>0.6766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.6786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.7898</td>\n",
       "      <td>0.6762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.7832</td>\n",
       "      <td>0.6789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.7788</td>\n",
       "      <td>0.6782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.7810</td>\n",
       "      <td>0.6775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.7826</td>\n",
       "      <td>0.6790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.7712</td>\n",
       "      <td>0.6769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.7707</td>\n",
       "      <td>0.6762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.7678</td>\n",
       "      <td>0.6770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.6754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.6763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.7751</td>\n",
       "      <td>0.6760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.7874</td>\n",
       "      <td>0.6778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.7766</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.6775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.6764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.7782</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.7962</td>\n",
       "      <td>0.6749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.6794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.7783</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.7751</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.7730</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.7729</td>\n",
       "      <td>0.6757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.7701</td>\n",
       "      <td>0.6782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.7784</td>\n",
       "      <td>0.6782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.7896</td>\n",
       "      <td>0.6766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.6809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.7793</td>\n",
       "      <td>0.6828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.7669</td>\n",
       "      <td>0.6775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.7783</td>\n",
       "      <td>0.6812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.7656</td>\n",
       "      <td>0.6803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.7707</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.7626</td>\n",
       "      <td>0.6804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.7642</td>\n",
       "      <td>0.6778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.7691</td>\n",
       "      <td>0.6772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.7676</td>\n",
       "      <td>0.6778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.7726</td>\n",
       "      <td>0.6789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.7788</td>\n",
       "      <td>0.6779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.7781</td>\n",
       "      <td>0.6778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.7856</td>\n",
       "      <td>0.6783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.6786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.6775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.7747</td>\n",
       "      <td>0.6774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.7737</td>\n",
       "      <td>0.6793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.7726</td>\n",
       "      <td>0.6804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.6797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.7685</td>\n",
       "      <td>0.6786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.7712</td>\n",
       "      <td>0.6795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.7794</td>\n",
       "      <td>0.6828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.7475</td>\n",
       "      <td>0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.6790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.7740</td>\n",
       "      <td>0.6850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.7796</td>\n",
       "      <td>0.6780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.6820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.7619</td>\n",
       "      <td>0.6805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.7614</td>\n",
       "      <td>0.6816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.6806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    generator_loss discriminator_loss\n",
       "0           2.0810             0.4177\n",
       "1           1.8224             0.4644\n",
       "2           1.5916             0.5411\n",
       "3           1.3012             0.5586\n",
       "4           1.5790             0.5100\n",
       "5           1.4779             0.5152\n",
       "6           1.3847             0.5257\n",
       "7           1.3238             0.5357\n",
       "8           1.3393             0.5537\n",
       "9           1.2527             0.5678\n",
       "10          1.2043             0.5874\n",
       "11          1.2488             0.5770\n",
       "12          1.1574             0.5916\n",
       "13          1.1015             0.5935\n",
       "14          1.1394             0.5964\n",
       "15          1.0812             0.6058\n",
       "16          1.0605             0.6108\n",
       "17          1.0493             0.6177\n",
       "18          1.0400             0.6150\n",
       "19          1.0527             0.6205\n",
       "20          1.0911             0.6178\n",
       "21          0.9498             0.6342\n",
       "22          0.9671             0.6284\n",
       "23          0.9533             0.6339\n",
       "24          0.9759             0.6259\n",
       "25          0.9975             0.6341\n",
       "26          0.9701             0.6350\n",
       "27          0.9300             0.6449\n",
       "28          0.9662             0.6344\n",
       "29          0.9488             0.6390\n",
       "30          0.9827             0.6318\n",
       "31          0.9534             0.6426\n",
       "32          0.9723             0.6394\n",
       "33          0.9265             0.6419\n",
       "34          0.9300             0.6403\n",
       "35          0.9239             0.6471\n",
       "36          0.9146             0.6443\n",
       "37          0.9454             0.6492\n",
       "38          0.9530             0.6530\n",
       "39          0.9238             0.6555\n",
       "40          0.8412             0.6561\n",
       "41          0.8565             0.6508\n",
       "42          0.8733             0.6480\n",
       "43          0.8896             0.6481\n",
       "44          0.9086             0.6467\n",
       "45          0.8915             0.6592\n",
       "46          0.8835             0.6542\n",
       "47          0.8722             0.6517\n",
       "48          0.9235             0.6509\n",
       "49          0.9769             0.6639\n",
       "50          0.8833             0.6557\n",
       "51          0.8528             0.6618\n",
       "52          0.8350             0.6627\n",
       "53          0.8559             0.6567\n",
       "54          0.8373             0.6621\n",
       "55          0.8586             0.6563\n",
       "56          0.8868             0.6616\n",
       "57          0.8635             0.6579\n",
       "58          0.8374             0.6684\n",
       "59          0.8000             0.6617\n",
       "60          0.8482             0.6591\n",
       "61          0.8966             0.6543\n",
       "62          0.8733             0.6651\n",
       "63          0.8143             0.6704\n",
       "64          0.8194             0.6691\n",
       "65          0.8325             0.6659\n",
       "66          0.8166             0.6719\n",
       "67          0.8374             0.6608\n",
       "68          0.8344             0.6652\n",
       "69          0.8232             0.6653\n",
       "70          0.8260             0.6630\n",
       "71          0.8412             0.6576\n",
       "72          0.8381             0.6610\n",
       "73          0.8666             0.6588\n",
       "74          0.8519             0.6627\n",
       "75          0.8661             0.6657\n",
       "76          0.7947             0.6737\n",
       "77          0.8235             0.6670\n",
       "78          0.8318             0.6629\n",
       "79          0.8259             0.6652\n",
       "80          0.8207             0.6637\n",
       "81          0.8252             0.6637\n",
       "82          0.8356             0.6585\n",
       "83          0.8555             0.6656\n",
       "84          0.8195             0.6689\n",
       "85          0.8353             0.6668\n",
       "86          0.8300             0.6675\n",
       "87          0.8213             0.6693\n",
       "88          0.8194             0.6664\n",
       "89          0.8231             0.6643\n",
       "90          0.8464             0.6639\n",
       "91          0.8166             0.6653\n",
       "92          0.8519             0.6624\n",
       "93          0.8448             0.6620\n",
       "94          0.8528             0.6635\n",
       "95          0.8435             0.6633\n",
       "96          0.8270             0.6645\n",
       "97          0.8229             0.6657\n",
       "98          0.8505             0.6601\n",
       "99          0.8272             0.6634\n",
       "100         0.8399             0.6641\n",
       "101         0.8186             0.6655\n",
       "102         0.8152             0.6651\n",
       "103         0.8354             0.6660\n",
       "104         0.8355             0.6664\n",
       "105         0.8573             0.6682\n",
       "106         0.8717             0.6647\n",
       "107         0.8279             0.6642\n",
       "108         0.8085             0.6671\n",
       "109         0.8317             0.6618\n",
       "110         0.8236             0.6671\n",
       "111         0.8322             0.6640\n",
       "112         0.8293             0.6675\n",
       "113         0.8439             0.6604\n",
       "114         0.8337             0.6614\n",
       "115         0.8350             0.6620\n",
       "116         0.8502             0.6626\n",
       "117         0.8365             0.6638\n",
       "118         0.8412             0.6658\n",
       "119         0.8266             0.6630\n",
       "120         0.8249             0.6652\n",
       "121         0.8327             0.6663\n",
       "122         0.8582             0.6588\n",
       "123         0.8273             0.6637\n",
       "124         0.8358             0.6625\n",
       "125         0.8462             0.6589\n",
       "126         0.8306             0.6626\n",
       "127         0.8290             0.6645\n",
       "128         0.8374             0.6642\n",
       "129         0.8422             0.6637\n",
       "130         0.8427             0.6599\n",
       "131         0.8665             0.6667\n",
       "132         0.8274             0.6688\n",
       "133         0.8109             0.6666\n",
       "134         0.8281             0.6645\n",
       "135         0.8408             0.6638\n",
       "136         0.8312             0.6692\n",
       "137         0.8317             0.6632\n",
       "138         0.8268             0.6682\n",
       "139         0.8224             0.6642\n",
       "140         0.8099             0.6658\n",
       "141         0.8249             0.6640\n",
       "142         0.8222             0.6669\n",
       "143         0.8312             0.6646\n",
       "144         0.8170             0.6646\n",
       "145         0.8262             0.6657\n",
       "146         0.8176             0.6669\n",
       "147         0.8389             0.6666\n",
       "148         0.8281             0.6659\n",
       "149         0.8210             0.6699\n",
       "150         0.8134             0.6650\n",
       "151         0.8290             0.6660\n",
       "152         0.8157             0.6687\n",
       "153         0.8277             0.6679\n",
       "154         0.8261             0.6654\n",
       "155         0.8194             0.6672\n",
       "156         0.8164             0.6666\n",
       "157         0.8344             0.6696\n",
       "158         0.8275             0.6747\n",
       "159         0.8078             0.6700\n",
       "160         0.8128             0.6718\n",
       "161         0.8246             0.6644\n",
       "162         0.8404             0.6677\n",
       "163         0.8093             0.6683\n",
       "164         0.8096             0.6685\n",
       "165         0.8035             0.6682\n",
       "166         0.8145             0.6681\n",
       "167         0.8302             0.6639\n",
       "168         0.8128             0.6686\n",
       "169         0.8130             0.6672\n",
       "170         0.8111             0.6675\n",
       "171         0.8272             0.6664\n",
       "172         0.8422             0.6652\n",
       "173         0.8232             0.6659\n",
       "174         0.8167             0.6671\n",
       "175         0.8129             0.6700\n",
       "176         0.8415             0.6629\n",
       "177         0.8222             0.6668\n",
       "178         0.8176             0.6673\n",
       "179         0.8182             0.6668\n",
       "180         0.8162             0.6667\n",
       "181         0.8333             0.6643\n",
       "182         0.8156             0.6672\n",
       "183         0.8151             0.6687\n",
       "184         0.8037             0.6692\n",
       "185         0.8214             0.6684\n",
       "186         0.8083             0.6684\n",
       "187         0.8156             0.6690\n",
       "188         0.8029             0.6706\n",
       "189         0.8257             0.6670\n",
       "190         0.8201             0.6696\n",
       "191         0.8338             0.6666\n",
       "192         0.8239             0.6707\n",
       "193         0.8165             0.6714\n",
       "194         0.8140             0.6738\n",
       "195         0.7896             0.6737\n",
       "196         0.8036             0.6723\n",
       "197         0.7995             0.6721\n",
       "198         0.8114             0.6722\n",
       "199         0.7965             0.6743\n",
       "200         0.7932             0.6689\n",
       "201         0.8036             0.6712\n",
       "202         0.7949             0.6687\n",
       "203         0.8050             0.6690\n",
       "204         0.8051             0.6724\n",
       "205         0.7976             0.6726\n",
       "206         0.8057             0.6695\n",
       "207         0.8073             0.6706\n",
       "208         0.8092             0.6703\n",
       "209         0.8165             0.6744\n",
       "210         0.8007             0.6726\n",
       "211         0.7926             0.6704\n",
       "212         0.8029             0.6704\n",
       "213         0.8023             0.6705\n",
       "214         0.7999             0.6700\n",
       "215         0.8027             0.6706\n",
       "216         0.8124             0.6695\n",
       "217         0.8173             0.6721\n",
       "218         0.7916             0.6762\n",
       "219         0.8025             0.6740\n",
       "220         0.7883             0.6748\n",
       "221         0.7944             0.6694\n",
       "222         0.7977             0.6704\n",
       "223         0.8047             0.6696\n",
       "224         0.8006             0.6705\n",
       "225         0.8030             0.6696\n",
       "226         0.8034             0.6721\n",
       "227         0.8052             0.6700\n",
       "228         0.8087             0.6716\n",
       "229         0.8187             0.6714\n",
       "230         0.7971             0.6741\n",
       "231         0.8143             0.6721\n",
       "232         0.7906             0.6773\n",
       "233         0.7868             0.6737\n",
       "234         0.7979             0.6761\n",
       "235         0.7883             0.6738\n",
       "236         0.7846             0.6727\n",
       "237         0.8002             0.6735\n",
       "238         0.8012             0.6712\n",
       "239         0.8098             0.6713\n",
       "240         0.8092             0.6746\n",
       "241         0.8048             0.6722\n",
       "242         0.7924             0.6725\n",
       "243         0.7962             0.6754\n",
       "244         0.7865             0.6754\n",
       "245         0.7999             0.6723\n",
       "246         0.7952             0.6740\n",
       "247         0.7954             0.6723\n",
       "248         0.7925             0.6723\n",
       "249         0.7834             0.6747\n",
       "250         0.7899             0.6720\n",
       "251         0.8004             0.6748\n",
       "252         0.8020             0.6727\n",
       "253         0.8095             0.6769\n",
       "254         0.7940             0.6747\n",
       "255         0.7812             0.6769\n",
       "256         0.7779             0.6768\n",
       "257         0.7891             0.6722\n",
       "258         0.7867             0.6750\n",
       "259         0.7914             0.6746\n",
       "260         0.7915             0.6770\n",
       "261         0.7837             0.6770\n",
       "262         0.7888             0.6755\n",
       "263         0.7821             0.6760\n",
       "264         0.7837             0.6771\n",
       "265         0.7847             0.6734\n",
       "266         0.7908             0.6765\n",
       "267         0.7936             0.6767\n",
       "268         0.7833             0.6765\n",
       "269         0.7725             0.6766\n",
       "270         0.7880             0.6760\n",
       "271         0.7918             0.6761\n",
       "272         0.7849             0.6772\n",
       "273         0.7887             0.6756\n",
       "274         0.7912             0.6761\n",
       "275         0.7904             0.6736\n",
       "276         0.7953             0.6776\n",
       "277         0.7849             0.6789\n",
       "278         0.8070             0.6746\n",
       "279         0.7864             0.6793\n",
       "280         0.7729             0.6786\n",
       "281         0.7711             0.6765\n",
       "282         0.7821             0.6750\n",
       "283         0.7977             0.6744\n",
       "284         0.7841             0.6746\n",
       "285         0.7809             0.6778\n",
       "286         0.7999             0.6775\n",
       "287         0.7835             0.6795\n",
       "288         0.7583             0.6776\n",
       "289         0.7792             0.6742\n",
       "290         0.7818             0.6762\n",
       "291         0.7797             0.6739\n",
       "292         0.7764             0.6753\n",
       "293         0.7850             0.6737\n",
       "294         0.7827             0.6746\n",
       "295         0.7814             0.6765\n",
       "296         0.7850             0.6747\n",
       "297         0.7800             0.6748\n",
       "298         0.7736             0.6784\n",
       "299         0.7972             0.6735\n",
       "300         0.7925             0.6776\n",
       "301         0.7809             0.6807\n",
       "302         0.7821             0.6769\n",
       "303         0.7701             0.6768\n",
       "304         0.7745             0.6781\n",
       "305         0.7812             0.6751\n",
       "306         0.7978             0.6783\n",
       "307         0.7928             0.6772\n",
       "308         0.7845             0.6792\n",
       "309         0.7889             0.6766\n",
       "310         0.7897             0.6757\n",
       "311         0.7870             0.6796\n",
       "312         0.7816             0.6774\n",
       "313         0.7749             0.6776\n",
       "314         0.7769             0.6766\n",
       "315         0.7804             0.6777\n",
       "316         0.7725             0.6765\n",
       "317         0.7773             0.6776\n",
       "318         0.7812             0.6756\n",
       "319         0.7793             0.6777\n",
       "320         0.7846             0.6781\n",
       "321         0.7765             0.6774\n",
       "322         0.7949             0.6740\n",
       "323         0.7833             0.6782\n",
       "324         0.7905             0.6743\n",
       "325         0.7701             0.6762\n",
       "326         0.7782             0.6762\n",
       "327         0.7831             0.6757\n",
       "328         0.7780             0.6773\n",
       "329         0.7931             0.6780\n",
       "330         0.7788             0.6806\n",
       "331         0.7705             0.6784\n",
       "332         0.7673             0.6773\n",
       "333         0.7755             0.6760\n",
       "334         0.7838             0.6741\n",
       "335         0.7887             0.6759\n",
       "336         0.7736             0.6785\n",
       "337         0.7802             0.6772\n",
       "338         0.7910             0.6810\n",
       "339         0.7630             0.6782\n",
       "340         0.7781             0.6797\n",
       "341         0.7692             0.6789\n",
       "342         0.7848             0.6766\n",
       "343         0.7842             0.6786\n",
       "344         0.7898             0.6762\n",
       "345         0.7832             0.6789\n",
       "346         0.7788             0.6782\n",
       "347         0.7810             0.6775\n",
       "348         0.7826             0.6790\n",
       "349         0.7712             0.6769\n",
       "350         0.7707             0.6762\n",
       "351         0.7678             0.6770\n",
       "352         0.7864             0.6754\n",
       "353         0.7797             0.6763\n",
       "354         0.7751             0.6760\n",
       "355         0.7874             0.6778\n",
       "356         0.7766             0.6772\n",
       "357         0.7698             0.6775\n",
       "358         0.7825             0.6764\n",
       "359         0.7782             0.6800\n",
       "360         0.7962             0.6749\n",
       "361         0.7760             0.6794\n",
       "362         0.7783             0.6772\n",
       "363         0.7751             0.6772\n",
       "364         0.7730             0.6772\n",
       "365         0.7729             0.6757\n",
       "366         0.7701             0.6782\n",
       "367         0.7784             0.6782\n",
       "368         0.7896             0.6766\n",
       "369         0.7842             0.6809\n",
       "370         0.7793             0.6828\n",
       "371         0.7669             0.6775\n",
       "372         0.7783             0.6812\n",
       "373         0.7656             0.6803\n",
       "374         0.7707             0.6800\n",
       "375         0.7626             0.6804\n",
       "376         0.7642             0.6778\n",
       "377         0.7691             0.6772\n",
       "378         0.7676             0.6778\n",
       "379         0.7726             0.6789\n",
       "380         0.7788             0.6779\n",
       "381         0.7781             0.6778\n",
       "382         0.7856             0.6783\n",
       "383         0.7700             0.6786\n",
       "384         0.7657             0.6775\n",
       "385         0.7747             0.6774\n",
       "386         0.7737             0.6793\n",
       "387         0.7726             0.6804\n",
       "388         0.7717             0.6797\n",
       "389         0.7685             0.6786\n",
       "390         0.7712             0.6795\n",
       "391         0.7794             0.6828\n",
       "392         0.7475             0.6800\n",
       "393         0.7779             0.6790\n",
       "394         0.7740             0.6850\n",
       "395         0.7796             0.6780\n",
       "396         0.7514             0.6820\n",
       "397         0.7619             0.6805\n",
       "398         0.7614             0.6816\n",
       "399         0.7653             0.6806"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('GAN_output.txt', 'r') as f:\n",
    "    f_str = f.readlines()\n",
    "    \n",
    "discrim_losses = []\n",
    "generator_losses = []\n",
    "for line in f_str:\n",
    "    if line[:19] == 'Discriminator Loss:':\n",
    "        discrim_losses.append(line[20:26])\n",
    "    elif line[:15] == 'Generator Loss:':\n",
    "        generator_losses.append(line[20:26])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data = {\n",
    "    'generator_loss' : generator_losses,\n",
    "    'discriminator_loss' : discrim_losses,\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the generator loss goes down, while the discriminator loss goes up. This makes sense, because the generator wants to fool the discriminator (i.e. increase the discriminator's loss), while the discriminator wants to minimize loss by classifying the images as correctly as possible. Looks like both achieved their respective goals.\n",
    "\n",
    "Mode collapse appears to be an easy way out for the generator--if it produces an \"especially plausible output, the generator may learn to produce *only* that output,\" since the generator just wants to find the fastest way to fool the discriminator. This might be similar to getting stuck in a local minimum, and the outputs would all end up being the same. Looking at the outputs, it seems that we avoided mode collapse. Here, I show the very results from the first epoch and the results from the last epoch. It looks like we have all the numbers represented, and just from eye-balling, it looks like the distribution of numbers if fairly even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gan_epoch_1.jpg\" width=\"500\"/>\n",
    "<img src=\"gan_epoch_400.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9\n",
    "\n",
    "I really enjoyed these exercises. I think I got to learn more about the 'state of the art' stuff, and this was my first exposure to Keras / TensorFlow, and it's so cool! For future improvements, I thought it would be interesting to look at more datasets, beyond the MNIST datset, especially for the GAN exercise. I also thought it was a little repetitive to add every single learning curve, so maybe in the future we could be asked to select only a few learning curves that were really interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
