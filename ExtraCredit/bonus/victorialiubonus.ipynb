{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 156a Extra Credit\n",
    "#### Victoria Liu\n",
    "#### November 20, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't end up using cloud computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running from the command line, I decided to place the code in a Jupyter so I could better loop over the number of neurons I wanted to test for each layer; there might be a way to do this on the command line as well, but I'm just more familiar with Jupyter Notebooks (incidentally, this homework is typed on a Jupyter Notebook as well). I got the following accuracies / number of parameters for each of the values I tested. I put everything in a dataframe for ease of reading. In the interest of keeping the notebook tidy, in the future, I will take out most of the code related to the creation of the dataframe when converting the notebook to pdf. I just wanted to show it for the first problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9685</td>\n",
       "      <td>32685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>32940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>33195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>33450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9706</td>\n",
       "      <td>34327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>40785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>41090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>41395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>41700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>44835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9695</td>\n",
       "      <td>45165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>45495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>45825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9734</td>\n",
       "      <td>56985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>57390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>57795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>58200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9811</td>\n",
       "      <td>178110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>241410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>400</td>\n",
       "      <td>800</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>642810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer1  layer2  accuracy  parameters\n",
       "0       40      25    0.9685       32685\n",
       "1       40      30    0.9687       32940\n",
       "2       40      35    0.9699       33195\n",
       "3       40      40    0.9699       33450\n",
       "19      41      41    0.9706       34327\n",
       "4       50      25    0.9688       40785\n",
       "5       50      30    0.9705       41090\n",
       "6       50      35    0.9732       41395\n",
       "7       50      40    0.9718       41700\n",
       "8       55      25    0.9715       44835\n",
       "9       55      30    0.9695       45165\n",
       "10      55      35    0.9704       45495\n",
       "11      55      40    0.9732       45825\n",
       "12      70      25    0.9734       56985\n",
       "13      70      30    0.9753       57390\n",
       "14      70      35    0.9752       57795\n",
       "15      70      40    0.9742       58200\n",
       "18     200     100    0.9811      178110\n",
       "16     200     400    0.9819      241410\n",
       "17     400     800    0.9815      642810"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "layer1 = [40, 40, 40, 40, 50, 50, 50, 50, 55, 55, 55, 55, 70, 70, 70, 70, 200, 400, 200, 41]\n",
    "layer2 = [25, 30, 35, 40, 25, 30, 35, 40, 25, 30, 35, 40, 25, 30, 35, 40, 400, 800, 100, 41]\n",
    "accuracy = [0.9685, 0.9687, 0.9699, 0.9699, 0.9688, 0.9705, 0.9732, 0.9718, 0.9715, 0.9695,\n",
    "            0.9704, 0.9732, 0.9734, 0.9753, 0.9752, 0.9742, 0.9819, 0.9815, 0.9811, 0.9706]\n",
    "parameters = [32685, 32940, 33195, 33450, 40785, 41090, 41395, 41700, 44835, 45165, 45495, \n",
    "              45825, 56985, 57390, 57795, 58200, 241410, 642810, 178110, 34327]\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "    'layer1' : layer1,\n",
    "    'layer2' : layer2,\n",
    "    'accuracy' : accuracy,\n",
    "    'parameters' : parameters\n",
    "    }\n",
    "    )\n",
    "df = df.sort_values(by=['parameters'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the learning curves. The models are titled as following: \"dense_2layers_*layer1*_*layer2*_learn.jpg\"\n",
    "\n",
    "<img src=\"dense_2layers_40_25_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_40_30_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_40_35_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_40_40_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_50_25_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_50_30_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_50_35_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_50_40_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_55_25_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_55_30_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_55_35_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_55_40_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_70_25_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_70_30_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_70_35_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_70_40_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_200_400_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_400_800_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_41_41_learn.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curves are as expected--as we feed in more epochs, the training and validation accuracy both get better, but at one point the training accuracy gets better because the model may be slightly over-fitting. Luckily, the overfitting doesn't seem to be a huge issue, since the validation and training errors are still close for most of the parameters. I noticed that for smaller numbers of parameters, the difference between training / validation errors is less, which makes sense wrt to the VC dimension. Interestingly, even when we have $642810$ parameters, although the difference between training and validation is quite great, the validation accuracy is still the highest out of all the models. However, it seems like no matter how many neurons we add, we can never seem to get $100$% accuracy, which is as expected.\n",
    "\n",
    "\n",
    "Overall, it seems like the more neurons we add to either layer, the better the accuracy gets. For example, if we keep the number of neurons in the first layer constant and add more neurons in the second layer, the validation accuracy will always increase, and vice versa for keeping the second layer constant. Interestingly, it seems like it might be a good idea to keep the number of neurons in the first and second layers similar; otherwise, the number of parameters will skyrocket. For example, when we have $41$ neurons in both layers, we get $0.9706$ accuracy and $34327$ parameters. When we have a comparable number of total neurons but distributed in $50$ vs $30$, we get $0.9705$ accuracy but $41090$ parameters. Also, the smallest number of parameters for which I could get over $97$ accuracy was around $34327$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out a few regularizer strengths and put them in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regularizer</th>\n",
       "      <th>validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.9811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.9824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.9784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.9798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.9806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.9784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.9792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.000000e-03</td>\n",
       "      <td>0.9626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.9179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regularizer  validation accuracy\n",
       "7  0.000000e+00               0.9811\n",
       "6  1.000000e-10               0.9824\n",
       "2  1.000000e-08               0.9784\n",
       "1  1.000000e-06               0.9798\n",
       "3  1.000000e-05               0.9806\n",
       "4  1.000000e-04               0.9784\n",
       "8  1.000000e-03               0.9792\n",
       "9  3.000000e-03               0.9626\n",
       "5  1.000000e-02               0.9179\n",
       "0  1.000000e+00               0.1135"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be too much issue with overfitting, since no regularizer actually did almost just as well as having a regularizer. However, there is clearly an underfitting issue for $\\lambda > 0.003$, since we see a drastic fall in validation accuracy there. It seems like having $\\lambda = 0.003$ gives around $96%$ to $97%$ accuracy, similar to what we saw in the smaller neural nets.\n",
    "\n",
    "The learning curves are as follows. As expected, the training and validation curves get further apart as the regularizer decreases, because we are fitting more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dense_reg_0_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_2_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_3_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_4_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_5_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_6_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_8_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_10_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_reg_0.003_learn.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the activation $\\theta = \\tanh(s)$. From class, we saw that $\\frac{d\\theta}{ds} = 1 - \\theta^{2}(s)$. As $|s| \\rightarrow \\infty$, $\\theta^{2} \\rightarrow 1$, and $\\frac{d\\theta}{ds} \\rightarrow 0$. This is bad news for our learning, since:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_{i}^{(l - 1)} = \\sum^{d^{(l)}}_{j=1} \\delta^{(l)}_{j} \\cdot w_{ij}^{(l)} \\cdot \\frac{d\\theta}{ds}\n",
    "\\end{align}\n",
    "\n",
    "If our last term goes to zero, then our delta's could get very small, and weights will get updated very slowly, since:\n",
    "\n",
    "\\begin{align}\n",
    "w^{(l)}_{ij} = w^{(l)}_{ij} - \\eta x^{(l-1)}_{i} \\delta_{j}^{(l)} \n",
    "\\end{align}\n",
    "\n",
    "I could see this being an issue for the rate of convergence when using the $\\tanh$ function. On the other hand, ReLu wouldn't be as much of a problem because as long as $s$ is positive, the slope would always be $1$, so we wouldn't see $delta$ continuously getting smaller for large values of $s$. I'm assuming that for negative values of $s$, even though $\\frac{d\\theta}{ds}$ goes to $0$, the positives values of $s$ more than make up for it, so there is much faster convergence in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just play around with the number of layers and the architecture of the layers, and we keep the same regularizer type, regularization parameter (0.00001), and nonlinearity ReLu. When we order our dataframe by validation accuracy, we see that smaller neural networks seem to do better. In fact, our highest accuracy, at $.98$, was when we had a single layer with all $200$ neurons in that layer. It also seems that we do better when the first layer has more neurons than the second layer. For example, when we have $150$ in the first layer and $50$ in the second, we get $.9778$ accuracy; when the layer counts are flipped, we get $.9754$. This might be due to a bottleneck effect, where not having enough neurons in the first layer prevented enough useful information from passing into the second layer. We can somewhat verify this by seeing how badly we did when we had fewer than $40$ neurons in the first layer; the highest we got was $0.9677$, and when we only had $10$ neurons in the first layer, we got. $.9512$ accuracy. Now, let's take the four top performers and vary the other parameters to see how they affect the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>layers</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>architecture</th>\n",
       "      <th>regularization_type</th>\n",
       "      <th>reg_param</th>\n",
       "      <th>nonlinearity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9512</td>\n",
       "      <td>4</td>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 60, 60, 70]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.9549</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>[20, 20, 20, 20, 20, 20, 20, 20, 20, 20]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9665</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[25, 25, 150]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9677</td>\n",
       "      <td>5</td>\n",
       "      <td>adam</td>\n",
       "      <td>[40, 40, 40, 40, 40]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9725</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[50, 100, 50]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9733</td>\n",
       "      <td>4</td>\n",
       "      <td>adam</td>\n",
       "      <td>[50, 50, 50, 50]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9735</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[70, 70, 60]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9745</td>\n",
       "      <td>4</td>\n",
       "      <td>adam</td>\n",
       "      <td>[70, 60, 60, 10]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9754</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>[50, 150]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9761</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[150, 25, 25]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9763</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9772</td>\n",
       "      <td>3</td>\n",
       "      <td>adam</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9778</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9801</td>\n",
       "      <td>1</td>\n",
       "      <td>adam</td>\n",
       "      <td>[200]</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_accuracy  layers optimizer                              architecture  \\\n",
       "9         0.9512       4      adam                          [10, 60, 60, 70]   \n",
       "11        0.9549      10      adam  [20, 20, 20, 20, 20, 20, 20, 20, 20, 20]   \n",
       "5         0.9665       3      adam                             [25, 25, 150]   \n",
       "10        0.9677       5      adam                      [40, 40, 40, 40, 40]   \n",
       "4         0.9725       3      adam                             [50, 100, 50]   \n",
       "8         0.9733       4      adam                          [50, 50, 50, 50]   \n",
       "7         0.9735       3      adam                              [70, 70, 60]   \n",
       "13        0.9745       4      adam                          [70, 60, 60, 10]   \n",
       "2         0.9754       2      adam                                 [50, 150]   \n",
       "6         0.9761       3      adam                             [150, 25, 25]   \n",
       "1         0.9763       2      adam                                [100, 100]   \n",
       "3         0.9772       3      adam                             [100, 50, 50]   \n",
       "12        0.9778       2      adam                                 [150, 50]   \n",
       "0         0.9801       1      adam                                     [200]   \n",
       "\n",
       "   regularization_type  reg_param nonlinearity  \n",
       "9                   l2    0.00001         relu  \n",
       "11                  l2    0.00001         relu  \n",
       "5                   l2    0.00001         relu  \n",
       "10                  l2    0.00001         relu  \n",
       "4                   l2    0.00001         relu  \n",
       "8                   l2    0.00001         relu  \n",
       "7                   l2    0.00001         relu  \n",
       "13                  l2    0.00001         relu  \n",
       "2                   l2    0.00001         relu  \n",
       "6                   l2    0.00001         relu  \n",
       "1                   l2    0.00001         relu  \n",
       "3                   l2    0.00001         relu  \n",
       "12                  l2    0.00001         relu  \n",
       "0                   l2    0.00001         relu  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>layers</th>\n",
       "      <th>architecture</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>regularization_type</th>\n",
       "      <th>reg_param</th>\n",
       "      <th>nonlinearity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0974</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>sgd</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>sgd</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0980</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>sgd</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.4877</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>sgd</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9705</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9722</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9732</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9743</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9763</td>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9772</td>\n",
       "      <td>3</td>\n",
       "      <td>[100, 50, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.9778</td>\n",
       "      <td>2</td>\n",
       "      <td>[150, 50]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9801</td>\n",
       "      <td>1</td>\n",
       "      <td>[200]</td>\n",
       "      <td>adam</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_accuracy  layers   architecture optimizer regularization_type  \\\n",
       "15        0.0974       1          [200]       sgd                  l2   \n",
       "9         0.0980       3  [100, 50, 50]      adam                  l2   \n",
       "4         0.0980       2     [100, 100]      adam                  l1   \n",
       "5         0.0980       3  [100, 50, 50]      adam                  l1   \n",
       "6         0.0980       2      [150, 50]      adam                  l1   \n",
       "7         0.0980       1          [200]      adam                  l1   \n",
       "8         0.0980       2     [100, 100]      adam                  l2   \n",
       "14        0.0980       2      [150, 50]       sgd                  l2   \n",
       "10        0.0980       2      [150, 50]      adam                  l2   \n",
       "11        0.0980       1          [200]      adam                  l2   \n",
       "12        0.0980       2     [100, 100]       sgd                  l2   \n",
       "13        0.4877       3  [100, 50, 50]       sgd                  l2   \n",
       "1         0.9705       3  [100, 50, 50]      adam                  l2   \n",
       "3         0.9722       1          [200]      adam                  l2   \n",
       "0         0.9732       2     [100, 100]      adam                  l2   \n",
       "2         0.9743       2      [150, 50]      adam                  l2   \n",
       "1         0.9763       2     [100, 100]      adam                  l2   \n",
       "3         0.9772       3  [100, 50, 50]      adam                  l2   \n",
       "12        0.9778       2      [150, 50]      adam                  l2   \n",
       "0         0.9801       1          [200]      adam                  l2   \n",
       "\n",
       "    reg_param nonlinearity  \n",
       "15    0.00001         relu  \n",
       "9     0.00100         relu  \n",
       "4     0.00001         relu  \n",
       "5     0.00001         relu  \n",
       "6     0.00001         relu  \n",
       "7     0.00001         relu  \n",
       "8     0.00100         relu  \n",
       "14    0.00001         relu  \n",
       "10    0.00100         relu  \n",
       "11    0.00100         relu  \n",
       "12    0.00001         relu  \n",
       "13    0.00001         relu  \n",
       "1     0.00001      sigmoid  \n",
       "3     0.00001      sigmoid  \n",
       "0     0.00001      sigmoid  \n",
       "2     0.00001      sigmoid  \n",
       "1     0.00001         relu  \n",
       "3     0.00001         relu  \n",
       "12    0.00001         relu  \n",
       "0     0.00001         relu  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is again ordered by validation accuracy. Right off the bat, we can see that `l1` is a bad regularization method when the regularizer is $0.00001$, not even getting $10$% accuracy, which is pretty much just randomly guessing at that point. I guess I shouldn't be too surprised, but we get similarly low accuracies for `l2` when the regularizer is $0.001$, and the accuracies are the same as the accuracies for `l1` at regularizer strength $0.00001$. This is probably because `l1` is kind of like the square root of `l2`, and $0.001$ is the square root of $0.00001$ (not sure if this logic completely holds, but it made sense in my head...). In addition, we see that `sgd` actually does a pretty bad job compared to `adam`, with `sgd` failing to get above $50$% accuracy. Lastly, `ReLu` appears to out-perform `Sigmoid` activation at high levels of accuracy. This suggests that the difference between choosing `ReLu` vs `Sigmoid` may not cause as drastic a difference as choosing bad values for other parameters, but there is still a difference.\n",
    "\n",
    "\n",
    "When looking at the learning curves, I couldn't find any \"pattern\" for when the training vs validation errors track close together and when they track further apart. I did notice that the learning curve for our best contender (one layer with $200$ neurons, adam, l2, $0.00001$ regularizer strength, ReLu, $0.9801$ accuracy) had a pretty high training accuracy at almost $100$%, but the validation accuracy didn't track as closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dense_2layers_50-150_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_adam_relu_l1_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_sgd_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_150-25-25_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_adam_sigmoid_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_5layers_40-40-40-40-40_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_adam_relu_l1_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_4layers_50-50-50-50_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_adam_relu_.001_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_adam_sigmoid_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_adam_relu_.001_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_50-100-50_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_10layers_20-20-20-20-20-20-20-20-20-20_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_25-25-150_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_4layers_70-60-60-10_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_sgd_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_adam_sigmoid_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_adam_relu_l1_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_sgd_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_1layers_200_adam_sigmoid_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_70-70-60_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_adam_relu_l1_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_3layers_100-50-50_sgd_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_100-100_adam_relu_.001_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_4layers_10-60-60-70_adam_relu_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"dense_2layers_150-50_adam_relu_.001_learn.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the model summaries, the densely connected model uses $178,110$ paramteres to train, while the convolutional model uses just $7,240$ parameters. That is more than $25$ orders of magnitude in difference and great news for our VC dimension / generalizability according to Hoeffding. Here are the learning curves. We can see that our intuition was correct--the learning curves for the convolutional neural network track better after $10$ epochs than the learning curves for the densely connected network. The validation accuracy is $0.9874$, while the training accuracy is $0.9877$. Not only do the accuracies track better with out of sample, but the accuracy itself is also higher than the accuracy from the densely connected network. Earlier in problem 2, the absolute best validation accuracy we could get was $0.9815$, so we can conclusively say that in this particular learning scenario, convolutional neural networks worked better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dense_no_reg_learn.jpg\" width=\"500\"/>\n",
    "<img src=\"conv_vs_dense_learn.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n",
    "\n",
    "#### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L^{(D)}}{\\partial D(X)} &= \\frac{\\partial}{\\partial D(X)} \\left[ -\\frac{1}{2} \\mathbb{E}_{x\\sim p_{data}} \\left[ \\log{D(x)} \\right] - \\frac{1}{2} \\mathbb{E}_{x \\sim p_{model}} \\left[ 1 - \\log{D(x)} \\right] \\right] \\\\[1em]\n",
    "\\frac{\\partial L^{(D)}}{\\partial D(X)} &= -\\frac{1}{2} \\frac{\\partial}{\\partial D(X)} \\left[  \\mathbb{E}_{x\\sim p_{data}} \\left[ \\log{D(x)} \\right] \\right] - \\frac{1}{2} \\frac{\\partial}{\\partial D(X)} \\left[ \\mathbb{E}_{x \\sim p_{model}} \\left[ 1 - \\log{D(x)} \\right] \\right] \\\\[1em]\n",
    "\\frac{\\partial L^{(D)}}{\\partial D(X)} &= -\\frac{1}{2} \\int \\frac{1}{D(x)} p_{data}(x) + \\frac{1}{2} \\int \\frac{1}{1 - D(x)} p_{model}(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{2} \\int \\frac{1}{D(x)} p_{data}(x) &= \\frac{1}{2} \\int \\frac{1}{1 - D(x)} p_{model}(x) \\\\[1em]\n",
    "\\frac{p_{data}(x)}{D(x)} &= \\frac{p_{model}(x)}{1 - D(x)} \\\\[1em]\n",
    "p_{data}(x) - p_{data}(x) \\cdot D(x) &= p_{model}(x) \\cdot D(x) \\\\[1em]\n",
    "D(x) &= \\frac{p_{data}(x)}{p_{model}(x) + p_{data}(x)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3\n",
    "I do indeed get that solution. I think this makes sense, because the discriminator is trying to make the loss as large as possible, and it can do so by quantifying the \"probability\" of getting the data probability distribution, compared to all the inputs it could get, from either the model probability distribution or the data probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8\n",
    "\n",
    "It looks like we are alternatively training the generator and then the discriminator. The generator is trying to fool the discriminator, and the discriminator is trying to point out which images are generated and which images are real. Looking at the code, it seems like we are training them separately (i.e. not at the same time), and this seems to be implemented using `discriminator.trainable = False` vs `discriminator.trainable = True`. In other words, the discriminator weights are 'frozen' in the adversarial net, and only the generator weights are trained. At other times, we will do the opposite to train the discriminator. Let's extract all the loss values for the generator and the discriminator and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generator_loss</th>\n",
       "      <th>discriminator_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0810</td>\n",
       "      <td>0.4177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.8224</td>\n",
       "      <td>0.4644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5916</td>\n",
       "      <td>0.5411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3012</td>\n",
       "      <td>0.5586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5790</td>\n",
       "      <td>0.5100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.7796</td>\n",
       "      <td>0.6780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.6820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.7619</td>\n",
       "      <td>0.6805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.7614</td>\n",
       "      <td>0.6816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.6806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    generator_loss discriminator_loss\n",
       "0           2.0810             0.4177\n",
       "1           1.8224             0.4644\n",
       "2           1.5916             0.5411\n",
       "3           1.3012             0.5586\n",
       "4           1.5790             0.5100\n",
       "..             ...                ...\n",
       "395         0.7796             0.6780\n",
       "396         0.7514             0.6820\n",
       "397         0.7619             0.6805\n",
       "398         0.7614             0.6816\n",
       "399         0.7653             0.6806\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the generator loss goes down, while the discriminator loss goes up. This makes sense, because the generator wants to fool the discriminator (i.e. increase the discriminator's loss), while the discriminator wants to minimize loss by classifying the images as correctly as possible. Looks like both achieved their respective goals.\n",
    "\n",
    "Mode collapse appears to be an easy way out for the generator--if it produces an \"especially plausible output, the generator may learn to produce *only* that output,\" since the generator just wants to find the fastest way to fool the discriminator. This might be similar to getting stuck in a local minimum, and the outputs would all end up being the same. Looking at the outputs, it seems that we avoided mode collapse. Here, I show the very results from the first epoch and the results from the last epoch. It looks like we have all the numbers represented, and just from eye-balling, it looks like the distribution of numbers if fairly even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gan_epoch_1.jpg\" width=\"500\"/>\n",
    "<img src=\"gan_epoch_400.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9\n",
    "\n",
    "I really enjoyed these exercises. I think I got to learn more about the 'state of the art' stuff, and this was my first exposure to Keras / TensorFlow, and it's so cool! For future improvements, I thought it would be interesting to look at more datasets, beyond the MNIST datset, especially for the GAN exercise. I also thought it was a little repetitive to add every single learning curve, so maybe in the future we could be asked to select only a few learning curves that were really interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
