{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Questions 1-5\\n\\nfor k in range(3, 8):\\n    print (\"k:\", k, \":\", experiment(k))\\n\\nprint (\\'\\n\\')\\n\\nfor k in range(3, 8):\\n    print (\"k:\", k, \":\", reverse_experiment(k))\\n\\n#output\\nk: 3 : (0.44, 0.3, 0.42)\\nk: 4 : (0.32, 0.5, 0.416)\\nk: 5 : (0.08, 0.2, 0.188)\\nk: 6 : (0.04, 0.0, 0.084)\\nk: 7 : (0.04, 0.1, 0.072)\\n\\nk: 3 : (0.4, 0.28, 0.396)\\nk: 4 : (0.3, 0.36, 0.388)\\nk: 5 : (0.2, 0.2, 0.284)\\nk: 6 : (0.0, 0.08, 0.192)\\nk: 7 : (0.0, 0.12, 0.196)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_inout_data(filename):\n",
    "    file = open(filename)\n",
    "    full_data = []\n",
    "    for line in file:\n",
    "        full_data.append(\n",
    "            [float(e) for e in line.split()]\n",
    "        )\n",
    "    full_data = np.asarray(full_data)\n",
    "    return full_data\n",
    "\n",
    "\n",
    "def split_training_validation(full_data):\n",
    "    training_set = full_data[:25]\n",
    "    validation_set = full_data[25:]\n",
    "    return (training_set, validation_set)\n",
    "\n",
    "\n",
    "def x_to_z_space(full_data, k):\n",
    "    Z_space = np.zeros ((len(full_data), k + 1))\n",
    "    Y = np.zeros(len(full_data))\n",
    "    \n",
    "    for i, (x1, x2, sign) in enumerate(full_data):\n",
    "        Z_space[i] = (1, x1, x2, x1**2, x2**2, x1 * x2, abs(x1 - x2), abs(x1 + x2))[:k+1]\n",
    "        Y[i] = sign\n",
    "    return (Z_space, Y)\n",
    "\n",
    "\n",
    "def g_weight(Z_space, Y):\n",
    "    _, len_g_weights = Z_space.shape\n",
    "    g_weights = np.zeros(len_g_weights)\n",
    "    g_weights = np.dot(np.linalg.pinv(Z_space), Y)\n",
    "    return g_weights\n",
    "\n",
    "\n",
    "def g_classify(Z_space, g_weights):\n",
    "    g_classification = [\n",
    "        float(sign) for sign in np.sign(np.dot(Z_space, g_weights))\n",
    "    ]\n",
    "    \n",
    "    g_classification = np.asarray(g_classification)\n",
    "    return g_classification\n",
    "\n",
    "\n",
    "def error_freq(g_classification, Y):\n",
    "    error_sum = np.sum(g_classification != Y)\n",
    "    error = error_sum / len(Y)\n",
    "    return error\n",
    "\n",
    "\n",
    "def experiment(k):\n",
    "    in_dta = get_inout_data(\"in.txt\")\n",
    "    out_dta = get_inout_data(\"out.txt\")\n",
    "\n",
    "    training, validation = split_training_validation(in_dta)\n",
    "        \n",
    "    z_training, y_training = x_to_z_space(training, k)\n",
    "    z_validation, y_validation = x_to_z_space(validation, k)\n",
    "    z_out, y_out = x_to_z_space(out_dta, k)\n",
    "    \n",
    "    g_weights = g_weight(z_training, y_training)\n",
    "    \n",
    "    training_classification = g_classify(z_training, g_weights)\n",
    "    validation_classification = g_classify(z_validation, g_weights)\n",
    "    out_classification = g_classify(z_out, g_weights)\n",
    "    \n",
    "    training_error = error_freq(training_classification, y_training)\n",
    "    validation_error = error_freq(validation_classification, y_validation)\n",
    "    out_error = error_freq(out_classification, y_out)\n",
    "    \n",
    "    return (training_error, validation_error, out_error)\n",
    "\n",
    "def reverse_experiment(k):\n",
    "    in_dta = get_inout_data(\"in.txt\")\n",
    "    out_dta = get_inout_data(\"out.txt\")\n",
    "\n",
    "    validation, training = split_training_validation(in_dta)\n",
    "        \n",
    "    z_training, y_training = x_to_z_space(training, k)\n",
    "    z_validation, y_validation = x_to_z_space(validation, k)\n",
    "    z_out, y_out = x_to_z_space(out_dta, k)\n",
    "    \n",
    "    g_weights = g_weight(z_training, y_training)\n",
    "    \n",
    "    training_classification = g_classify(z_training, g_weights)\n",
    "    validation_classification = g_classify(z_validation, g_weights)\n",
    "    out_classification = g_classify(z_out, g_weights)\n",
    "    \n",
    "    training_error = error_freq(training_classification, y_training)\n",
    "    validation_error = error_freq(validation_classification, y_validation)\n",
    "    out_error = error_freq(out_classification, y_out)\n",
    "    \n",
    "    return (training_error, validation_error, out_error)\n",
    "\n",
    "'''\n",
    "#Questions 1-5\n",
    "\n",
    "for k in range(3, 8):\n",
    "    print (\"k:\", k, \":\", experiment(k))\n",
    "\n",
    "print ('\\n')\n",
    "\n",
    "for k in range(3, 8):\n",
    "    print (\"k:\", k, \":\", reverse_experiment(k))\n",
    "\n",
    "#output\n",
    "k: 3 : (0.44, 0.3, 0.42)\n",
    "k: 4 : (0.32, 0.5, 0.416)\n",
    "k: 5 : (0.08, 0.2, 0.188)\n",
    "k: 6 : (0.04, 0.0, 0.084)\n",
    "k: 7 : (0.04, 0.1, 0.072)\n",
    "\n",
    "k: 3 : (0.4, 0.28, 0.396)\n",
    "k: 4 : (0.3, 0.36, 0.388)\n",
    "k: 5 : (0.2, 0.2, 0.284)\n",
    "k: 6 : (0.0, 0.08, 0.192)\n",
    "k: 7 : (0.0, 0.12, 0.196)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 100 \n",
      "\n",
      "run: 200 \n",
      "\n",
      "run: 300 \n",
      "\n",
      "run: 400 \n",
      "\n",
      "run: 500 \n",
      "\n",
      "run: 600 \n",
      "\n",
      "run: 700 \n",
      "\n",
      "run: 800 \n",
      "\n",
      "run: 900 \n",
      "\n",
      "run: 1000 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.997"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sklearn.svm\n",
    "import math\n",
    "\n",
    "def generate_point(boundary1, boundary2, dimension):\n",
    "    \"\"\"\n",
    "    Generate random two-dimensional point on \n",
    "    [boundary1, boundary 2] X [boundary1, boundary2] space.\n",
    "    Returns ndarray of (x, y) point\n",
    "    \"\"\"\n",
    "    random_point = np.zeros(dimension)\n",
    "    for i in range(dimension):\n",
    "        random_point[i] = np.random.uniform(boundary1, boundary2, 1)\n",
    "    return random_point\n",
    "\n",
    "#question 6\n",
    "def expected_min_value(boundary1, boundary2, runs):\n",
    "    \"\"\"\n",
    "    Let e1 and e2 be independent random variables, distributed uniformly over the\n",
    "    interval [boundary1, boundary2]. The function determines expected value of \n",
    "    e_min = min(e1, e2)\n",
    "    \n",
    "    Inputs:\n",
    "        boundary1 (float or int)\n",
    "        boundary2 (float or int)\n",
    "        runs (int) number of run times for determining expected value\n",
    "        \n",
    "    Outputs:\n",
    "        e_min_expected (float)\n",
    "    \"\"\"\n",
    "    e_min_tally = 0\n",
    "    for run in range(runs):\n",
    "        e1 = generate_point(boundary1, boundary2, 1)[0]\n",
    "        e2 = generate_point(boundary1, boundary2, 1)[0]\n",
    "        e = min(e1, e2)\n",
    "        e_min_tally += e\n",
    "    \n",
    "    e_min_expected = e_min_tally / runs\n",
    "    return e_min_expected\n",
    "    \n",
    "\n",
    "def generate_target_f():\n",
    "    \"\"\"\n",
    "    Returns slope and intercept of line connecting two random points\n",
    "    \"\"\"\n",
    "    point_1 = generate_point(-1, 1, 2)\n",
    "    point_2 = generate_point(-1, 1, 2)\n",
    "    \n",
    "    # slope = (y2 - y1) / (x2 - x1)\n",
    "    slope = (point_2[1] - point_1[1]) / (point_2[0] - point_1[0])\n",
    "    # intercept = y1 - slope * x1\n",
    "    intercept = point_2[1] - slope * point_2[0]\n",
    "    return(slope, intercept)\n",
    "\n",
    "\n",
    "def classify_point(random_point, slope, intercept):\n",
    "    \"\"\"\n",
    "    Given random_point in (x, y) form and a slope and intercept,\n",
    "    label the point\n",
    "    +1 if it falls above the line\n",
    "    -1 if it falls below the line\n",
    "    \"\"\"\n",
    "    if random_point[1] > slope * random_point[0] + intercept:\n",
    "        classification = 1\n",
    "    else:\n",
    "        classification = -1\n",
    "    return classification\n",
    "\n",
    "def create_training_data(N, slope, intercept, svm = False):\n",
    "    \"\"\"\n",
    "    Creates N points for training data using target f(X) = slope * X + intercept;\n",
    "    Notes:\n",
    "        Target function f is a line connecting two points in X space [-1, 1] x [-1, 1]\n",
    "        Classification is based on whether points lie above or below f\n",
    "        Excludes data sets where all points lie in the same region\n",
    "        \n",
    "    Inputs:\n",
    "        N (int)\n",
    "    Outputs:\n",
    "        X_training (ndarray), X_training.shape = (N, 3); x0 = 1\n",
    "        Y_training (ndarray), Y_training.shsape = (N, )\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # create matrix X, where x0 is always 1, to accomodate w0;\n",
    "        #if svm == True, we will take out the x0 later\n",
    "        X_training = np.ones((N, 3))\n",
    "        Y_training = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            random_point = generate_point(-1, 1, 2)\n",
    "            classification = classify_point(random_point, slope, intercept)\n",
    "            X_training[i, 1:3] = random_point\n",
    "            Y_training[i] = classification\n",
    "        \n",
    "        #make sure that the points don't all lie on the same side of the line\n",
    "        if abs(np.sum(Y_training)) != N:\n",
    "            #svm does not use x0\n",
    "            #ok so turns out we need to use the same test data for pla and svm, so we never got to use svm = True\n",
    "            #but keeping this in anyway, since it's more inclusive\n",
    "            if svm == True:\n",
    "                X_training = X_training[:, 1:3]\n",
    "            return (X_training, Y_training)\n",
    "\n",
    "def create_testing_data(N, slope, intercept, svm = False):\n",
    "    \"\"\"\n",
    "    Creates N points for testing data using target f(X) = slope * X + intercept;\n",
    "    Notes:\n",
    "        Target function f is a line connecting two points in X space [-1, 1] x [-1, 1]\n",
    "        Classification is based on whether points lie above or below f\n",
    "        Excludes data sets where all points lie in the same region\n",
    "        \n",
    "    Inputs:\n",
    "        N (int)\n",
    "    Outputs:\n",
    "        X_testing (ndarray), X_testing.shape = (N, 3); x0 = 1\n",
    "        Y_testing (ndarray), Y_testing.shsape = (N, )\n",
    "    \"\"\"\n",
    "    # create matrix X, where x0 is always 1, to accomodate w0\n",
    "    #if svm == True, we will take out the x0 later\n",
    "    X_testing = np.ones((N, 3))\n",
    "    Y_testing = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        random_point = generate_point(-1, 1, 2)\n",
    "        classification = classify_point(random_point, slope, intercept)\n",
    "        X_testing[i, 1:3] = random_point\n",
    "        Y_testing[i] = classification\n",
    "        \n",
    "    #ok so turns out we need to use the same test data for pla and svm, so we never got to use svm = True\n",
    "    #but keeping this in anyway, since it's more inclusive\n",
    "    if svm == True:\n",
    "        X_testing = X_testing[:, 1:3]\n",
    "    return (X_testing, Y_testing)\n",
    "\n",
    "\n",
    "def perceptron(X_training, Y_training):\n",
    "    \"\"\"\n",
    "    Starts with the weight as a zero-vector.\n",
    "    Changes the weight when it meets any misclassified point.\n",
    "    Returns the new weight once all the points are correctly clasified.\n",
    "    \n",
    "    Inputs:\n",
    "        X_training (ndarray), complete input features of all points\n",
    "        Y_training (ndarray), complete training classification of all points\n",
    "        \n",
    "    Outputs:\n",
    "        weight (ndarray), final hypothesis g(X) = Y for perceptron learning\n",
    "        \n",
    "    \"\"\"\n",
    "    feature_length = X_training.shape[1]\n",
    "    weight = np.zeros(feature_length)\n",
    "\n",
    "    while True:\n",
    "        misclassified_point_count = 0\n",
    "        for i in range(len(X_training)):\n",
    "            if isit_misclassified_point(X_training[i], Y_training[i], weight) == True:\n",
    "                weight = adjust_weight(X_training[i], Y_training[i], weight)\n",
    "                misclassified_point_count += 1\n",
    "        if misclassified_point_count == 0:\n",
    "            break\n",
    "            \n",
    "    return weight\n",
    "\n",
    "def perceptron_adjustment_per_iteration(X_training, Y_training):\n",
    "    \"\"\"\n",
    "    Starts with the weight as a zero-vector.\n",
    "    In a given iteration, checks every single point for misclassification.\n",
    "    At the end of the iteration, randomly chooses a misclassified point for weight adjustment.\n",
    "    Returns the new weight once all the points are correctly classified.\n",
    "    \n",
    "    Inputs:\n",
    "        X_training (ndarray), complete input features of all points\n",
    "        Y_training (ndarray), complete training classification of all points\n",
    "        \n",
    "    Outputs:\n",
    "        weight (ndarray), final hypothesis g(X) = Y for perceptron learning\n",
    "        \n",
    "    \"\"\"\n",
    "    feature_length = X_training.shape[1]\n",
    "    weight = np.zeros(feature_length)\n",
    "\n",
    "    while True:\n",
    "        misclassified_points = []\n",
    "        for i in range(len(X_training)):\n",
    "            if isit_misclassified_point(X_training[i], Y_training[i], weight) == True:\n",
    "                misclassified_points.append(i)\n",
    "        if len(misclassified_points) == 0:\n",
    "            break\n",
    "            \n",
    "        random_i = random.choice(misclassified_points)\n",
    "        weight = adjust_weight(X_training[random_i], Y_training[random_i], weight)\n",
    "        \n",
    "    return weight\n",
    "\n",
    "def adjust_weight(x_misclassified, y_misclassified, weight):\n",
    "    \"\"\"\n",
    "    Given a single misclassified point and the current weight vector,\n",
    "    adjust the weight to accomodate our misclassifed point.\n",
    "    \n",
    "    Inputs:\n",
    "        x_misclassified (ndarray)\n",
    "        y_misclassified (+/- 1)\n",
    "        weight (ndarray)\n",
    "        \n",
    "    Outputs:\n",
    "        misclassified (boolean)\n",
    "    \"\"\"\n",
    "    adjusted_weight = weight + np.dot(y_misclassified, x_misclassified)\n",
    "    return adjusted_weight\n",
    "\n",
    "\n",
    "def isit_misclassified_point(x, y, weight):\n",
    "    \"\"\"\n",
    "    Given a single point (i.e. vector x and label y)\n",
    "    and a weight that we are currently training or testing, it will determine whether the\n",
    "    current hypothesis weight correctly or incorrectly classifies the point.\n",
    "    Tests hypothesis g(x) = sign (weight . x) == y for classification\n",
    "    \n",
    "    Inputs:\n",
    "        x (ndarray)\n",
    "        y (+/- 1)\n",
    "        weight (must be ndarray)\n",
    "        \n",
    "    Outputs:\n",
    "        misclassified (boolean)\n",
    "    \n",
    "    \"\"\"\n",
    "    if np.sign(np.dot(weight.T, x)) != y:\n",
    "        miclassified = True\n",
    "    else:\n",
    "        miclassified = False\n",
    "    return miclassified\n",
    "\n",
    "def perceptron_error_single_run(X_testing, Y_testing, weight):\n",
    "    \"\"\"\n",
    "    g = (weight . x)\n",
    "    Inputs:\n",
    "        X_testing (ndarray), complete input features of testing set\n",
    "        Y_testing (ndarray), complete accurate classification of testing set, from f(x)\n",
    "        weight (ndarray), generated by perceptron() func\n",
    "    Outputs:\n",
    "        pla_error_freq (float), P[f(x)!= g_pla(x)]\n",
    "    \"\"\"\n",
    "    \n",
    "    N = X_testing.shape[0]\n",
    "    error_count = 0\n",
    "    for i in range(N):\n",
    "        if isit_misclassified_point(X_testing[i], Y_testing[i], weight):\n",
    "            error_count += 1\n",
    "            \n",
    "    pla_error_freq = error_count / N\n",
    "    return pla_error_freq\n",
    "    \n",
    "    \n",
    "def svm_error(support_vector_machine, X_testing, Y_testing):\n",
    "    svm_error_freq = 1 - support_vector_machine.score(X_testing, Y_testing)\n",
    "    return svm_error_freq\n",
    "    \n",
    "def experiment_error_comparison(N_training, N_testing, runs, kernel = 'linear', C = math.inf):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        N: (int), number of points\n",
    "        runs: (int), number of runs (i.e. # comparisons between svm vs. pla OR\n",
    "                                    # of svm runs\n",
    "                                    )\n",
    "    \"\"\"\n",
    "    #create svm better than pla comparison tally\n",
    "    svm_better_tally = 0\n",
    "    \n",
    "    for i in range(runs):\n",
    "        if (i + 1) % 100 == 0:  \n",
    "            print (\"run:\", i + 1, \"\\n\")\n",
    "            \n",
    "        #generate random function f(X) for every run\n",
    "        slope, intercept = generate_target_f()\n",
    "        \n",
    "        #create training data based on f(X)\n",
    "        X_training, Y_training = create_training_data(N_training, slope, intercept)\n",
    "        \n",
    "        #get weights for best hypothesis g\n",
    "        weight = perceptron_adjustment_per_iteration(X_training, Y_training)\n",
    "        \n",
    "        #create testing data based on f(X)\n",
    "        X_testing, Y_testing = create_testing_data(N_testing, slope, intercept)\n",
    "        \n",
    "        #use perceptron_error_single_run to get pla_error_freq\n",
    "        pla_error_freq = perceptron_error_single_run(X_testing, Y_testing, weight)\n",
    "        \n",
    "\n",
    "        #train/ fit svm\n",
    "        support_vector_machine = sklearn.svm.SVC(kernel = kernel, C = C)\n",
    "        support_vector_machine.fit(X_training[:, 1:3], Y_training) #svm does not use x0\n",
    "        \n",
    "        #svm error_freq\n",
    "        svm_error_freq = svm_error(support_vector_machine, X_testing[:, 1:3], Y_testing)\n",
    "        \n",
    "        #compare svm error_freq and pla_error_freq\n",
    "        if svm_error_freq < pla_error_freq:\n",
    "            svm_better_tally += 1\n",
    "            \n",
    "    #outside forloop\n",
    "    #svm better than pla comparison tally / runs\n",
    "    svm_better_freq = svm_better_tally / runs\n",
    "    \n",
    "    return svm_better_freq\n",
    "\n",
    "def experiment_svm_count(N, runs, kernel = 'linear', C = math.inf):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        N: (int), number of training points to generate & svm fit\n",
    "        runs: (int), number of svm runs, to average over\n",
    "        kernel = \"linear\"\n",
    "        C = infinity for hard boundary\n",
    "    \"\"\"\n",
    "    #initialize svm tally\n",
    "    svm_tally = 0\n",
    "    \n",
    "    for i in range(runs):\n",
    "        if (i + 1) % 100 == 0:  \n",
    "            print (\"run:\", i + 1, \"\\n\")\n",
    "            \n",
    "        #generate random function f(X) for every run\n",
    "        slope, intercept = generate_target_f()\n",
    "        \n",
    "        #create training data based on f(X)\n",
    "        X_training, Y_training = create_training_data(N, slope, intercept)\n",
    "        \n",
    "        #train/ fit svm\n",
    "        support_vector_machine = sklearn.svm.SVC(kernel = kernel, C = C)\n",
    "        support_vector_machine.fit(X_training[:, 1:3], Y_training) #svm does not use x0\n",
    "        \n",
    "        #get number of support vectors / update svm tally\n",
    "        svm_single_run_count = len(support_vector_machine.support_)\n",
    "        svm_tally += svm_single_run_count\n",
    "        \n",
    "    #outside forloop, find average svm_count\n",
    "    svm_count_average = svm_tally / runs\n",
    "    \n",
    "    return svm_count_average\n",
    "\n",
    "#question 6\n",
    "#expected_min_value(0, 1, 10000) #result: .334\n",
    "\n",
    "#question 8\n",
    "#experiment_error_comparison(10, 10000, 1000, kernel = 'linear', C = math.inf) # result: .601\n",
    "\n",
    "#question 9\n",
    "#experiment_error_comparison(100, 10000, 1000, kernel = 'linear', C = math.inf) #result: .629\n",
    "\n",
    "#question 10\n",
    "#experiment_svm_count(100, 1000, kernel = 'linear', C = math.inf) #result: 2.996\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
